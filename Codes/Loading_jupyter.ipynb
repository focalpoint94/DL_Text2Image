{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br> Final Proejct: Text to Image Synthesis (Tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) Data Science & AI Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For understanding of this work, please carefully look at given PPT file.**\n",
    "\n",
    "**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the training process **</font> so that TAs can grade both your code and results.  \n",
    "**The TA will set a config file as 'eval_birds.yml' when evaluating the code using 'hidden test dataset'. Thus, please make sure that your code can generate proper data to measure inception score and R-precision of 'hidden test dataset'.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load datasets\n",
    "The Birds dataset will be downloaded automatically if it is not located in the *data* directory. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented for: main_1215_b + EDSR_NoiseAugmentation + CNN_ENCODER_256, RNN_ENCODER_256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chszerg/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/chszerg/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/chszerg/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/chszerg/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/chszerg/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/chszerg/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os, nltk\n",
    "from miscc.config import cfg, cfg_from_file\n",
    "import pprint\n",
    "import datetime\n",
    "import dateutil.tz\n",
    "import numpy as np\n",
    "import scipy\n",
    "from utils.data_utils import CUBDataset\n",
    "from utils.loss import cosine_similarity\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "\n",
    "#################################################\n",
    "# DO NOT CHANGE \n",
    "from utils.model_ import CNN_ENCODER, RNN_ENCODER, GENERATOR, DISCRIMINATOR, CNN_ENCODER_256, RNN_ENCODER_256\n",
    "#################################################\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using config:\n",
      "{'BATCH_SIZE': 64,\n",
      " 'CHECKPOINT_DIR': './checkpoint',\n",
      " 'CHECKPOINT_NAME': 'model.ckpt',\n",
      " 'CNN': {'EMBEDDING_DIM': 0, 'H_DIM': 0},\n",
      " 'CONFIG_NAME': 'text-to-image',\n",
      " 'CUDA': False,\n",
      " 'DATASET_NAME': 'birds',\n",
      " 'DATA_DIR': 'data/birds',\n",
      " 'EMBEDDING_TYPE': 'cnn-rnn',\n",
      " 'GAN': {'B_ATTENTION': False,\n",
      "         'B_CONDITION': False,\n",
      "         'B_DCGAN': False,\n",
      "         'CONDITION_DIM': 0,\n",
      "         'DF_DIM': 0,\n",
      "         'EMBEDDING_DIM': 0,\n",
      "         'GF_DIM': 0,\n",
      "         'R_NUM': 0,\n",
      "         'Z_DIM': 512},\n",
      " 'GPU_ID': '0',\n",
      " 'IMAGE_SIZE': 256,\n",
      " 'NUM_BATCH_FOR_TEST': 0,\n",
      " 'RANDOM_SEED': 0,\n",
      " 'RNN': {'EMBEDDING_DIM': 0,\n",
      "         'H_DIM': 0,\n",
      "         'TYPE': '',\n",
      "         'VOCAB_SIZE': 0,\n",
      "         'WORD_EMBEDDING_DIM': 0},\n",
      " 'R_PRECISION_DIR': './evaluation',\n",
      " 'R_PRECISION_FILE': 'r_precision.npz',\n",
      " 'R_PRECISION_FILE_HIDDEN': 'r_precision_hidden.npz',\n",
      " 'TEST': {'B_EXAMPLE': False,\n",
      "          'GENERATED_HIDDEN_TEST_IMAGES': './evaluation/generated_images_hidden',\n",
      "          'GENERATED_TEST_IMAGES': './evaluation/generated_images'},\n",
      " 'TEXT': {'CAPTIONS_PER_IMAGE': 10, 'EMBEDDING_DIM': 128, 'WORDS_NUM': 20},\n",
      " 'TRAIN': {'CNN_ENCODER': '',\n",
      "           'COEFF': {'COLOR_LOSS': 0.0, 'KL': 0.0, 'UNCOND_LOSS': 0.0},\n",
      "           'DISCRIMINATOR': '',\n",
      "           'DISCRIMINATOR_LR': 0.0,\n",
      "           'FLAG': True,\n",
      "           'GENERATOR': '',\n",
      "           'GENERATOR_LR': 0.0,\n",
      "           'MAX_EPOCH': 600,\n",
      "           'RNN_ENCODER': '',\n",
      "           'SNAPSHOT_INTERVAL': 0},\n",
      " 'WORKERS': 4,\n",
      " 'WRONG_CAPTION': 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chszerg/final-project-deep-learning-19-tf/miscc/config.py:121: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  yaml_cfg = edict(yaml.load(f))\n"
     ]
    }
   ],
   "source": [
    "# Set a config file as 'train_birds.yml' in training, as 'eval_birds.yml' for evaluation\n",
    "cfg_from_file('cfg/train_birds.yml') # eval_birds.yml\n",
    "\n",
    "print('Using config:')\n",
    "pprint.pprint(cfg)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = cfg.GPU_ID\n",
    "\n",
    "now = datetime.datetime.now(dateutil.tz.tzlocal())\n",
    "timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
    "output_dir = 'sample/%s_%s_%s' % (cfg.DATASET_NAME, cfg.CONFIG_NAME, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.current_dir:\n",
      "/home/chszerg/final-project-deep-learning-19-tf\n",
      "\n",
      "self.data_dir:\n",
      "/home/chszerg/final-project-deep-learning-19-tf/data/birds\n",
      "\n",
      "self.image_dir:\n",
      "/home/chszerg/final-project-deep-learning-19-tf/data/birds/CUB_200_2011.tgz\n",
      "\n",
      "Dataset already exists\n",
      "self.image_dir:\n",
      "/home/chszerg/final-project-deep-learning-19-tf/data/birds/CUB_200_2011/images\n",
      "\n",
      "Load from:  data/birds/captions.pickle\n",
      "self.current_dir:\n",
      "/home/chszerg/final-project-deep-learning-19-tf\n",
      "\n",
      "self.data_dir:\n",
      "/home/chszerg/final-project-deep-learning-19-tf/data/birds\n",
      "\n",
      "self.image_dir:\n",
      "/home/chszerg/final-project-deep-learning-19-tf/data/birds/CUB_200_2011.tgz\n",
      "\n",
      "Dataset already exists\n",
      "self.image_dir:\n",
      "/home/chszerg/final-project-deep-learning-19-tf/data/birds/CUB_200_2011/images\n",
      "\n",
      "Load from:  data/birds/captions.pickle\n",
      "\n",
      "train data directory:\n",
      "/home/chszerg/final-project-deep-learning-19-tf/data/birds/train\n",
      "test data directory:\n",
      "/home/chszerg/final-project-deep-learning-19-tf/data/birds/test\n",
      "\n",
      "# of train filenames:(8855,)\n",
      "# of test filenames:(2933,)\n",
      "\n",
      "example of filename of train image:002.Laysan_Albatross/Laysan_Albatross_0002_1027\n",
      "example of filename of valid image:001.Black_footed_Albatross/Black_Footed_Albatross_0046_18\n",
      "\n",
      "example of caption and its ids:\n",
      "['a', 'bird', 'with', 'a', 'very', 'long', 'wing', 'span', 'and', 'a', 'long', 'pointed', 'beak']\n",
      "[ 1  2  3  1  4  5  6  7  8  1  5  9 10  0  0  0  0  0  0  0]\n",
      "\n",
      "example of caption and its ids:\n",
      "['light', 'tan', 'colored', 'bird', 'with', 'a', 'white', 'head', 'and', 'an', 'orange', 'beak']\n",
      "[ 67 106  89   2   3   1  14  25   8  28  52  10   0   0   0   0   0   0\n",
      "   0   0]\n",
      "\n",
      "# of train captions:(88550,)\n",
      "# of test captions:(29330,)\n",
      "\n",
      "# of train caption ids:(88550, 20)\n",
      "# of test caption ids:(29330, 20)\n",
      "\n",
      "# of train images:(8855, 256, 256, 3)\n",
      "# of test images:(2933, 256, 256, 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CUBDataset(cfg.DATA_DIR, split='train')\n",
    "test_dataset = CUBDataset(cfg.DATA_DIR, split='test')\n",
    "\n",
    "print(f'\\ntrain data directory:\\n{train_dataset.split_dir}')\n",
    "print(f'test data directory:\\n{test_dataset.split_dir}\\n')\n",
    "\n",
    "print(f'# of train filenames:{train_dataset.filenames.shape}')\n",
    "print(f'# of test filenames:{test_dataset.filenames.shape}\\n')\n",
    "\n",
    "print(f'example of filename of train image:{train_dataset.filenames[0]}')\n",
    "print(f'example of filename of valid image:{test_dataset.filenames[0]}\\n')\n",
    "\n",
    "print(f'example of caption and its ids:\\n{train_dataset.captions[0]}\\n{train_dataset.captions_ids[0]}\\n')\n",
    "print(f'example of caption and its ids:\\n{test_dataset.captions[0]}\\n{test_dataset.captions_ids[0]}\\n')\n",
    "\n",
    "print(f'# of train captions:{np.asarray(train_dataset.captions).shape}')\n",
    "print(f'# of test captions:{np.asarray(test_dataset.captions).shape}\\n')\n",
    "\n",
    "print(f'# of train caption ids:{np.asarray(train_dataset.captions_ids).shape}')\n",
    "print(f'# of test caption ids:{np.asarray(test_dataset.captions_ids).shape}\\n')\n",
    "\n",
    "print(f'# of train images:{train_dataset.images.shape}')\n",
    "print(f'# of test images:{test_dataset.images.shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8855, 256, 256, 3)\n",
      "(2933, 256, 256, 3)\n",
      "(88550, 20)\n",
      "(29330, 20)\n"
     ]
    }
   ],
   "source": [
    "train_images = train_dataset.images\n",
    "test_images = test_dataset.images\n",
    "train_captions = np.asarray(train_dataset.captions_ids)\n",
    "test_captions = np.asarray(test_dataset.captions_ids)\n",
    "print(train_images.shape)\n",
    "print(test_images.shape)\n",
    "print(train_captions.shape)\n",
    "print(test_captions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8855, 64, 64, 3)\n",
      "(2933, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "from skimage.transform import resize\n",
    "train_images_64 = []\n",
    "for train_image in train_images:\n",
    "    train_images_64.append(resize(train_image, (64, 64, 3)))\n",
    "train_images_64 = np.asarray(train_images_64)\n",
    "print(train_images_64.shape)\n",
    "assert train_images_64.shape[0] == train_images.shape[0]\n",
    "test_images_64 = []\n",
    "for test_image in test_images:\n",
    "    test_images_64.append(resize(test_image, (64, 64, 3)))\n",
    "test_images_64 = np.asarray(test_images_64)\n",
    "print(test_images_64.shape)\n",
    "assert test_images_64.shape[0] == test_images.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_captions_train = len(train_captions)\n",
    "n_captions_per_image = 10\n",
    "n_images_train = len(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "import threading\n",
    "import scipy.ndimage as ndi\n",
    "from skimage import transform\n",
    "from skimage import exposure\n",
    "import skimage\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def sent2ID(sample_sentence):\n",
    "    caption = []\n",
    "    cap = sample_sentence\n",
    "    if len(cap) == 0:\n",
    "        exit()\n",
    "    cap = cap.replace(\"\\ufffd\\ufffd\", \" \")\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(cap.lower())\n",
    "    tokens_new = []\n",
    "    for t in tokens:\n",
    "        t = t.encode('ascii', 'ignore').decode('ascii')\n",
    "        if len(t) > 0:\n",
    "            tokens_new.append(t)\n",
    "    caption.append(tokens_new)\n",
    "    caption_new = []\n",
    "    t = caption[0]\n",
    "    rev = []\n",
    "    for w in t:\n",
    "        if w in train_dataset.wordtoix:\n",
    "            rev.append(train_dataset.wordtoix[w])\n",
    "    x, x_len = train_dataset.get_caption(rev)\n",
    "    caption_new.append(np.squeeze(x, axis=1))\n",
    "    return caption_new\n",
    "\n",
    "def ID2sent(sample_caption):\n",
    "    sentence = []\n",
    "    for ID in sample_caption:\n",
    "        if ID != train_dataset.ixtoword['<PAD>']:\n",
    "            sentence.append(train_dataset.ixtoword[ID])\n",
    "    return sentence\n",
    "\n",
    "def get_random_int(min=0, max=10, number=5):\n",
    "    return [random.randint(min,max) for p in range(0,number)]\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "    return img\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    return scipy.misc.imsave(path, merge(images, size))\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(images, size, image_path)\n",
    "\n",
    "def threading_data(data=None, fn=None, **kwargs):\n",
    "    def apply_fn(results, i, data, kwargs):\n",
    "        results[i] = fn(data, **kwargs)\n",
    "    results = [None] * len(data)\n",
    "    threads = []\n",
    "    for i in range(len(data)):\n",
    "        t = threading.Thread(\n",
    "                        name='threading_and_return',\n",
    "                        target=apply_fn,\n",
    "                        args=(results, i, data[i], kwargs)\n",
    "                        )\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    return np.asarray(results)\n",
    "\n",
    "def threading_data_256(data=None, fn=None, **kwargs):\n",
    "    def apply_fn(results, i, data, kwargs):\n",
    "        results[i] = fn(data, **kwargs)\n",
    "    results = [None] * len(data)\n",
    "    threads = []\n",
    "    for i in range(len(data)):\n",
    "        t = threading.Thread(\n",
    "                        name='threading_and_return',\n",
    "                        target=apply_fn,\n",
    "                        args=(results, i, data[i], kwargs)\n",
    "                        )\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    return np.asarray(results)\n",
    "\n",
    "def apply_transform(x, transform_matrix, channel_index=2, fill_mode='nearest', cval=0., order=1):\n",
    "    x = np.rollaxis(x, channel_index, 0)\n",
    "    final_affine_matrix = transform_matrix[:2, :2]\n",
    "    final_offset = transform_matrix[:2, 2]\n",
    "    channel_images = [ndi.interpolation.affine_transform(x_channel, final_affine_matrix,\n",
    "                      final_offset, order=order, mode=fill_mode, cval=cval) for x_channel in x]\n",
    "    x = np.stack(channel_images, axis=0)\n",
    "    x = np.rollaxis(x, 0, channel_index + 1)\n",
    "    return x\n",
    "\n",
    "def transform_matrix_offset_center(matrix, x, y):\n",
    "    o_x = float(x) / 2 + 0.5\n",
    "    o_y = float(y) / 2 + 0.5\n",
    "    offset_matrix = np.array([[1, 0, o_x], [0, 1, o_y], [0, 0, 1]])\n",
    "    reset_matrix = np.array([[1, 0, -o_x], [0, 1, -o_y], [0, 0, 1]])\n",
    "    transform_matrix = np.dot(np.dot(offset_matrix, matrix), reset_matrix)\n",
    "    return transform_matrix\n",
    "\n",
    "def rotation(x, rg=20, is_random=False, row_index=0, col_index=1, channel_index=2,\n",
    "                    fill_mode='nearest', cval=0.):\n",
    "    if is_random:\n",
    "        theta = np.pi / 180 * np.random.uniform(-rg, rg)\n",
    "    else:\n",
    "        theta = np.pi / 180 * rg\n",
    "    rotation_matrix = np.array([[np.cos(theta), -np.sin(theta), 0],\n",
    "                                [np.sin(theta), np.cos(theta), 0],\n",
    "                                [0, 0, 1]])\n",
    "    h, w = x.shape[row_index], x.shape[col_index]\n",
    "    transform_matrix = transform_matrix_offset_center(rotation_matrix, h, w)\n",
    "    x = apply_transform(x, transform_matrix, channel_index, fill_mode, cval)\n",
    "    return x\n",
    "\n",
    "def crop(x, wrg, hrg, is_random=False, row_index=0, col_index=1, channel_index=2):\n",
    "    h, w = x.shape[row_index], x.shape[col_index]\n",
    "    assert (h > hrg) and (w > wrg), \"The size of cropping should smaller than the original image\"\n",
    "    if is_random:\n",
    "        h_offset = int(np.random.uniform(0, h-hrg) - 1)\n",
    "        w_offset = int(np.random.uniform(0, w-wrg) - 1)\n",
    "        return x[h_offset: hrg + h_offset ,w_offset: wrg + w_offset]\n",
    "    else:\n",
    "        h_offset = int(np.floor((h - hrg)/ 2.))\n",
    "        w_offset = int(np.floor((w - wrg)/ 2.))\n",
    "        h_end = h_offset + hrg\n",
    "        w_end = w_offset + wrg\n",
    "        return x[h_offset: h_end, w_offset: w_end]\n",
    "\n",
    "def flip_axis(x, axis, is_random=False):\n",
    "    if is_random:\n",
    "        factor = np.random.uniform(-1, 1)\n",
    "        if factor > 0:\n",
    "            x = np.asarray(x).swapaxes(axis, 0)\n",
    "            x = x[::-1, ...]\n",
    "            x = x.swapaxes(0, axis)\n",
    "            return x\n",
    "        else:\n",
    "            return x\n",
    "    else:\n",
    "        x = np.asarray(x).swapaxes(axis, 0)\n",
    "        x = x[::-1, ...]\n",
    "        x = x.swapaxes(0, axis)\n",
    "        return x\n",
    "\n",
    "def imresize(x, size=[100, 100], interp='bilinear', mode=None):\n",
    "    if x.shape[-1] == 1:\n",
    "        x = scipy.misc.imresize(x[:, :, 0], size, interp=interp, mode=mode)\n",
    "        return x[:, :, np.newaxis]\n",
    "    elif x.shape[-1] == 3:\n",
    "        return scipy.misc.imresize(x, size, interp=interp, mode=mode)\n",
    "    else:\n",
    "        raise Exception(\"Unsupported channel %d\" % x.shape[-1])\n",
    "\n",
    "def prepro_img(x, mode=None):\n",
    "    if mode=='train':\n",
    "        x = flip_axis(x, axis=1, is_random=True)\n",
    "        x = rotation(x, rg=16, is_random=True, fill_mode='nearest')\n",
    "        x = imresize(x, size=[64 + 15, 64 + 15], interp='bilinear', mode=None)\n",
    "        x = crop(x, wrg=64, hrg=64, is_random=True)\n",
    "        x = x / (255. / 2.)\n",
    "        x = x - 1.\n",
    "    return x\n",
    "\n",
    "def prepro_img_256(x, mode=None):\n",
    "    if mode=='train':\n",
    "        x = flip_axis(x, axis=1, is_random=True)\n",
    "        x = rotation(x, rg=16, is_random=True, fill_mode='nearest')\n",
    "        x = imresize(x, size=[256 + 60, 256 + 60], interp='bilinear', mode=None)\n",
    "        x = crop(x, wrg=256, hrg=256, is_random=True)\n",
    "        x = x / (255. / 2.)\n",
    "        x = x - 1.\n",
    "    return x\n",
    "\n",
    "def combine_and_save_image_sets(image_sets, directory):\n",
    "    for i in range(len(image_sets[0])):\n",
    "        combined_image = []\n",
    "        for set_no in range(len(image_sets)):\n",
    "            combined_image.append(image_sets[set_no][i])\n",
    "            combined_image.append(np.zeros((image_sets[set_no][i].shape[0], 5, 3)))\n",
    "        combined_image = np.concatenate(combined_image, axis = 1)\n",
    "        scipy.misc.imsave(os.path.join(directory, 'combined_{}.jpg'.format(i)), combined_image)\n",
    "\n",
    "def save_encoders_256(saver, sess, logdir, step):\n",
    "    model_name = 'encoders_256.ckpt'\n",
    "    checkpoint_path = os.path.join(logdir, model_name)\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "    saver.save(sess, checkpoint_path, global_step=step)\n",
    "    print('The checkpoint has been created.')\n",
    "        \n",
    "def load(saver, sess, ckpt_path):\n",
    "    saver.restore(sess, ckpt_path)\n",
    "    print(\"Restored model parameters from {}\".format(ckpt_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "def resBlock(x, channels=64, kernel_size=[3,3], scale=1):\n",
    "    tmp = slim.conv2d(x, channels, kernel_size, activation_fn=None)\n",
    "    tmp = tf.nn.relu(tmp)\n",
    "    tmp = slim.conv2d(tmp, channels, kernel_size, activation_fn=None)\n",
    "    tmp *= scale\n",
    "    return x + tmp\n",
    "\n",
    "def upsample(x, scale=2, features=64, activation=tf.nn.relu):\n",
    "    assert scale in [2,3,4]\n",
    "    x = slim.conv2d(x, features, [3,3], activation_fn=activation)\n",
    "    if scale == 2:\n",
    "        ps_features = 3*(scale**2)\n",
    "        x = slim.conv2d(x, ps_features, [3,3], activation_fn=activation)\n",
    "        x = PS(x, 2, color=True)\n",
    "    elif scale == 3:\n",
    "        ps_features =3*(scale**2)\n",
    "        x = slim.conv2d(x, ps_features, [3,3], activation_fn=activation)\n",
    "        x = PS(x, 3, color=True)\n",
    "    elif scale == 4:\n",
    "        ps_features = 3*(2**2)\n",
    "        for i in range(2):\n",
    "            x = slim.conv2d(x, ps_features, [3,3], activation_fn=activation)\n",
    "            x = PS(x, 2, color=True)\n",
    "    return x\n",
    "\n",
    "def _phase_shift(I, r):\n",
    "    bsize, a, b, c = I.get_shape().as_list()\n",
    "    bsize = tf.shape(I)[0]\n",
    "    X = tf.reshape(I, (bsize, a, b, r, r))\n",
    "    X = tf.transpose(X, (0, 1, 2, 4, 3))\n",
    "    X = tf.split(X, a, 1)\n",
    "    X = tf.concat([tf.squeeze(x, axis=1) for x in X],2)\n",
    "    X = tf.split(X, b, 1)\n",
    "    X = tf.concat([tf.squeeze(x, axis=1) for x in X],2)\n",
    "    return tf.reshape(X, (bsize, a*r, b*r, 1))\n",
    "\n",
    "def PS(X, r, color=False):\n",
    "    if color:\n",
    "        Xc = tf.split(X, 3, 3)\n",
    "        X = tf.concat([_phase_shift(x, r) for x in Xc],3)\n",
    "    else:\n",
    "        X = _phase_shift(X, r)\n",
    "    return X\n",
    "\n",
    "def log10(x):\n",
    "    numerator = tf.log(x)\n",
    "    denominator = tf.log(tf.constant(10, dtype=numerator.dtype))\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Text2Img and EDSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 20)\n"
     ]
    }
   ],
   "source": [
    "z_dim = 512\n",
    "batch_size = 64\n",
    "sample_size = batch_size\n",
    "ni = 8\n",
    "sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, z_dim)).astype(np.float32)\n",
    "sample_sentence = [\"a black bird with oily black feathers and rounded black beak.\"] * int(sample_size/ni) + \\\n",
    "                  [\"a medium sized black bird, with a white belly, and webbed feet.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this is a white bird with black webbed feet and a black beak.\"] * int(sample_size/ni) + \\\n",
    "                  [\"a small dully colored bird that has a grey head and nape, an oatmeal colored breast, belly and yellow and oatmeal-grey colored wings and tail.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this bird has a yellow throat, breast and belly, with a black band at the throat, and black crown, wings and tail.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this small bird is soft green all over, with a light eyering, short wings and moderate tail.\"] * int(sample_size/ni) + \\\n",
    "                  [\"small bird with crown and throat is yellow, outer and inner rectrices are grey, beak is small, black and pointed.\"] * int(sample_size/ni) + \\\n",
    "                  [\"a brown bird with striped wings, a large head, a long pointy beak, a long tail, and narrow legs.\"] * int(sample_size/ni)\n",
    "for i, sent in enumerate(sample_sentence):\n",
    "    sample_sentence[i] = sent2ID(sent)\n",
    "sample_sentence = np.asarray(sample_sentence)\n",
    "sample_sentence = np.reshape(sample_sentence, (sample_size, 20))\n",
    "print(sample_sentence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Text2Img:\n",
    "    def __init__(self):\n",
    "        \"\"\" Information \"\"\"\n",
    "        self.lr = 0.0002\n",
    "        self.lr_decay = 0.5      \n",
    "        self.decay_every = 100  \n",
    "        self.beta1 = 0.5\n",
    "        self.checkpoint_dir = './checkpoint'\n",
    "        self.z_dim = 512\n",
    "        self.image_size = 64\n",
    "        self.c_dim = 3\n",
    "        self.batch_size = 64\n",
    "        self.alpha = 0.2\n",
    "        \n",
    "        \"\"\" Place Holders \"\"\"\n",
    "        self.t_real_image = tf.placeholder('float32', [self.batch_size, self.image_size, self.image_size, 3], name = 'real_image')\n",
    "        self.t_wrong_image = tf.placeholder('float32', [self.batch_size ,self.image_size, self.image_size, 3], name = 'wrong_image')\n",
    "        self.t_real_caption = tf.placeholder(dtype=tf.int64, shape=[self.batch_size, None], name='real_caption_input')\n",
    "        self.t_wrong_caption = tf.placeholder(dtype=tf.int64, shape=[self.batch_size, None], name='wrong_caption_input')\n",
    "        self.t_z = tf.placeholder(tf.float32, [self.batch_size, self.z_dim], name='z_noise')\n",
    "        \n",
    "        \"\"\" Training Phase - CNN - RNN mapping \"\"\"\n",
    "        net_cnn = CNN_ENCODER(self.t_real_image, is_training=True, reuse=False)\n",
    "        x = net_cnn.outputs\n",
    "        v = RNN_ENCODER(self.t_real_caption, is_training=True, reuse=False).outputs\n",
    "        x_w = CNN_ENCODER(self.t_wrong_image, is_training=True, reuse=True).outputs\n",
    "        v_w = RNN_ENCODER(self.t_wrong_caption, is_training=True, reuse=True).outputs\n",
    "        rnn_loss = tf.reduce_mean(tf.maximum(0., self.alpha - cosine_similarity(x, v) + cosine_similarity(x, v_w))) + \\\n",
    "                    tf.reduce_mean(tf.maximum(0., self.alpha - cosine_similarity(x, v) + cosine_similarity(x_w, v)))\n",
    "        \n",
    "        \"\"\" Training Phase - GAN \"\"\"\n",
    "        net_rnn = RNN_ENCODER(self.t_real_caption, is_training=False, reuse=True)\n",
    "        net_fake_image = GENERATOR(self.t_z, net_rnn.outputs, is_training=True, reuse=False)\n",
    "        net_disc_fake = DISCRIMINATOR(net_fake_image.outputs, net_rnn.outputs, is_training=True, reuse=False)\n",
    "        disc_fake_logits = net_disc_fake.logits\n",
    "        net_disc_real = DISCRIMINATOR(self.t_real_image, net_rnn.outputs, is_training=True, reuse=True)\n",
    "        disc_real_logits = net_disc_real.logits\n",
    "        net_disc_mismatch = DISCRIMINATOR(self.t_real_image, RNN_ENCODER(self.t_wrong_caption, is_training=False, reuse=True).outputs,\n",
    "                                        is_training=True, reuse=True)\n",
    "        disc_mismatch_logits = net_disc_mismatch.logits\n",
    "        d_loss1 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_real_logits,     labels=tf.ones_like(disc_real_logits),      name='d1'))\n",
    "        d_loss2 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_mismatch_logits, labels=tf.zeros_like(disc_mismatch_logits), name='d2'))\n",
    "        d_loss3 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_fake_logits,     labels=tf.zeros_like(disc_fake_logits),     name='d3'))\n",
    "        d_loss = d_loss1 + (d_loss2 + d_loss3) * 0.5\n",
    "        g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_fake_logits, labels=tf.ones_like(disc_fake_logits), name='g'))\n",
    "\n",
    "        \"\"\" Testing Phase \"\"\"\n",
    "        self.net_g = GENERATOR(self.t_z, RNN_ENCODER(self.t_real_caption, is_training=False, reuse=True).outputs,\n",
    "                            is_training=False, reuse=True)\n",
    "        \n",
    "        \"\"\" Training \"\"\"\n",
    "        rnn_vars = [var for var in tf.trainable_variables() if 'rnnencoder' in var.name]\n",
    "        cnn_vars = [var for var in tf.trainable_variables() if 'cnnencoder' in var.name]\n",
    "        d_vars = [var for var in tf.trainable_variables() if 'discriminator' in var.name]\n",
    "        g_vars = [var for var in tf.trainable_variables() if 'generator' in var.name]\n",
    "        update_ops_CNN = [var for var in tf.get_collection(tf.GraphKeys.UPDATE_OPS) if 'cnnencoder' in var.name]\n",
    "        update_ops_D = [var for var in tf.get_collection(tf.GraphKeys.UPDATE_OPS) if 'discriminator' in var.name]\n",
    "        update_ops_G = [var for var in tf.get_collection(tf.GraphKeys.UPDATE_OPS) if 'generator' in var.name]\n",
    "        with tf.variable_scope('learning_rate'):\n",
    "            lr_v = tf.Variable(self.lr, trainable=False)\n",
    "        with tf.control_dependencies(update_ops_CNN):\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(rnn_loss, rnn_vars + cnn_vars), 10)\n",
    "            optimizer = tf.train.AdamOptimizer(lr_v, beta1=self.beta1)\n",
    "            rnn_optim = optimizer.apply_gradients(zip(grads, rnn_vars + cnn_vars))\n",
    "        with tf.control_dependencies(update_ops_D):\n",
    "            d_optim = tf.train.AdamOptimizer(lr_v, beta1=self.beta1).minimize(d_loss, var_list=d_vars)\n",
    "        with tf.control_dependencies(update_ops_G):\n",
    "            g_optim = tf.train.AdamOptimizer(lr_v, beta1=self.beta1).minimize(g_loss, var_list=g_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDSR(object):\n",
    "    def __init__(self, img_size=64, num_layers=32, feature_size=256, scale=4, output_channels=3, lr=1e-4):\n",
    "        \"\"\" Information \"\"\"\n",
    "        self.img_size = img_size\n",
    "        self.scale = scale\n",
    "        self.output_channels = output_channels\n",
    "        self.lr = tf.Variable(lr, trainable=False)\n",
    "        \n",
    "        \"\"\" Place Holders\"\"\"\n",
    "        self.input = x = tf.placeholder(tf.float32, [None, img_size, img_size, output_channels])\n",
    "        self.target = y = tf.placeholder(tf.float32,[None, img_size*scale, img_size*scale, output_channels])\n",
    "        image_input = x\n",
    "        image_target = y\n",
    "        \n",
    "        \"\"\" Graph \"\"\"\n",
    "        x = slim.conv2d(image_input, feature_size, [3,3])\n",
    "        conv_1 = x\n",
    "        scaling_factor = 0.1\n",
    "        for i in range(num_layers):\n",
    "            x = resBlock(x, feature_size, scale=scaling_factor)\n",
    "        x = slim.conv2d(x, feature_size, [3,3])\n",
    "        x += conv_1\n",
    "        x = upsample(x, scale, feature_size, None)\n",
    "        self.out = output = tf.nn.tanh(x)\n",
    "        \n",
    "        \"\"\" Loss and Optimizer \"\"\"\n",
    "        self.loss = loss = tf.reduce_mean(tf.losses.absolute_difference(image_target, output))\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_op = optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./64_gen_checkpoint/model.ckpt-950\n",
      "INFO:tensorflow:Restoring parameters from ./EDSR_checkpoint/model.ckpt-216\n"
     ]
    }
   ],
   "source": [
    "text2img_graph = tf.Graph()\n",
    "with text2img_graph.as_default():\n",
    "    text2img = Text2Img()\n",
    "\n",
    "edsr_graph = tf.Graph()\n",
    "with edsr_graph.as_default():\n",
    "    edsr = EDSR()\n",
    "\n",
    "text2img_sess = tf.Session(graph=text2img_graph)\n",
    "edsr_sess = tf.Session(graph=edsr_graph)\n",
    "\n",
    "text2img_checkpoint_dir = './64_gen_checkpoint'\n",
    "with text2img_sess.as_default():\n",
    "    with text2img_graph.as_default():\n",
    "        tf.global_variables_initializer().run()\n",
    "        text2img_saver = tf.train.Saver(tf.global_variables())\n",
    "        text2img_ckpt = tf.train.get_checkpoint_state(text2img_checkpoint_dir)\n",
    "        text2img_saver.restore(text2img_sess, text2img_ckpt.model_checkpoint_path)\n",
    "\n",
    "edsr_checkpoint_dir = './EDSR_checkpoint'\n",
    "with edsr_sess.as_default():\n",
    "    with edsr_graph.as_default():\n",
    "        tf.global_variables_initializer().run()\n",
    "        edsr_saver = tf.train.Saver(tf.global_variables())\n",
    "        edsr_ckpt = tf.train.get_checkpoint_state(edsr_checkpoint_dir)\n",
    "        edsr_saver.restore(edsr_sess, edsr_ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 64, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chszerg/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/ipykernel_launcher.py:53: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    }
   ],
   "source": [
    "sample_images = text2img_sess.run([text2img.net_g.outputs],\n",
    "                                  feed_dict={text2img.t_real_caption : sample_sentence, text2img.t_z : sample_seed})\n",
    "sample_images = np.asarray(sample_images)\n",
    "sample_images = np.reshape(sample_images, [64, 64, 64, 3])\n",
    "print(sample_images.shape)\n",
    "save_images(sample_images, [8, 8], 'temp/temp1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 256, 256, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chszerg/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/ipykernel_launcher.py:53: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    }
   ],
   "source": [
    "output_images = edsr_sess.run([edsr.out], feed_dict={edsr.input: sample_images})\n",
    "output_images = np.asarray(output_images)\n",
    "output_images.shape\n",
    "output_images = np.reshape(output_images, [64, 256, 256, 3])\n",
    "print(output_images.shape)\n",
    "save_images(output_images, [8, 8], 'temp/temp2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train CNN_ENCODER_256, RNN_ENCODER_256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENCODERS_256:\n",
    "    def __init__(self):\n",
    "        \"\"\" Information \"\"\"\n",
    "        self.batch_size = 64\n",
    "        self.alpha = 0.2\n",
    "        self.lr = 2e-4\n",
    "        self.beta1 = 0.5\n",
    "        \n",
    "        \"\"\" Place Holders \"\"\"\n",
    "        self.real_image_256 = tf.placeholder('float32', [self.batch_size, 256, 256, 3], name = 'real_image')\n",
    "        self.wrong_image_256 = tf.placeholder('float32', [self.batch_size, 256, 256, 3], name = 'fake_image')\n",
    "        self.real_caption_256 = tf.placeholder(dtype=tf.int64, shape=[self.batch_size, None], name='real_caption_input')\n",
    "        self.wrong_caption_256 = tf.placeholder(dtype=tf.int64, shape=[self.batch_size, None], name='wrong_caption_input')\n",
    "        self.t_z_256 = tf.placeholder(tf.float32, [self.batch_size, 512], name='z_noise')\n",
    "        \n",
    "        \"\"\" Training Phase - CNN - RNN mapping \"\"\"\n",
    "        net_cnn_256 = CNN_ENCODER_256(self.real_image_256, is_training=True, reuse=False)\n",
    "        x_256 = net_cnn_256.outputs\n",
    "        v_256 = RNN_ENCODER_256(self.real_caption_256, is_training=True, reuse=False).outputs\n",
    "        x_w_256 = CNN_ENCODER_256(self.wrong_image_256, is_training=True, reuse=True).outputs\n",
    "        v_w_256 = RNN_ENCODER_256(self.wrong_caption_256, is_training=True, reuse=True).outputs\n",
    "        self.rnn_loss_256 = tf.reduce_mean(tf.maximum(0., self.alpha - cosine_similarity(x_256, v_256) + cosine_similarity(x_256, v_w_256))) + \\\n",
    "                        tf.reduce_mean(tf.maximum(0., self.alpha - cosine_similarity(x_256, v_256) + cosine_similarity(x_w_256, v_256)))\n",
    "        \n",
    "        \"\"\" Training \"\"\"\n",
    "        rnn_vars_256 = [var for var in tf.trainable_variables() if 'rnn_256' in var.name]\n",
    "        cnn_vars_256 = [var for var in tf.trainable_variables() if 'cnn_256' in var.name]\n",
    "        update_ops_CNN_256 = [var for var in tf.get_collection(tf.GraphKeys.UPDATE_OPS) if 'cnn_256' in var.name]\n",
    "        with tf.variable_scope('learning_rate'):\n",
    "            self.lr_v_256 = tf.Variable(self.lr, trainable=False)\n",
    "        with tf.control_dependencies(update_ops_CNN_256):\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(self.rnn_loss_256, rnn_vars_256 + cnn_vars_256), 10.)\n",
    "            optimizer = tf.train.AdamOptimizer(self.lr_v_256, beta1=self.beta1)\n",
    "            self.rnn_optim_256 = optimizer.apply_gradients(zip(grads, rnn_vars_256 + cnn_vars_256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-4\n",
    "lr_decay = 0.5      \n",
    "decay_every = 80\n",
    "beta1 = 0.5\n",
    "checkpoint_dir = './checkpoint_encoders_256'\n",
    "z_dim = 512\n",
    "image_size = 256\n",
    "c_dim = 3\n",
    "batch_size = 64\n",
    "n_epoch = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model = ENCODERS_256()\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement = True\n",
    "sess = tf.Session(config=config)\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "saver = tf.train.Saver(var_list=tf.global_variables(), max_to_keep=10)\n",
    "ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "    loader = tf.train.Saver(var_list=tf.global_variables())\n",
    "    load_step = int(os.path.basename(ckpt.model_checkpoint_path).split('-')[1])\n",
    "    load(loader, sess, ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    print('no checkpoints find.')\n",
    "\n",
    "n_batch_epoch = int(n_images_train / batch_size)\n",
    "for epoch in range(n_epoch):\n",
    "    start_time = time.time()\n",
    "    if epoch !=0 and (epoch % decay_every == 0):\n",
    "        new_lr_decay = lr_decay ** (epoch // decay_every)\n",
    "        sess.run(tf.assign(model.lr_v_256, lr * new_lr_decay))\n",
    "        log = \" ** new learning rate: %f\" % (lr * new_lr_decay)\n",
    "        print(log)\n",
    "    elif epoch == 0:\n",
    "        log = \" ** init lr: %f  decay_every_epoch: %d, lr_decay: %f\" % (lr, decay_every, lr_decay)\n",
    "        print(log)\n",
    "    for step in range(n_batch_epoch):\n",
    "        step_time = time.time()\n",
    "        idexs = get_random_int(min=0, max=n_captions_train-1, number=batch_size)\n",
    "        b_real_caption = train_captions[idexs]\n",
    "        b_real_images = train_images[np.floor(np.asarray(idexs).astype('float') / n_captions_per_image).astype('int')]\n",
    "        idexs = get_random_int(min=0, max=n_captions_train-1, number=batch_size)\n",
    "        b_wrong_caption = train_captions[idexs]\n",
    "        idexs2 = get_random_int(min=0, max=n_images_train-1, number=batch_size)\n",
    "        b_wrong_images = train_images[idexs2]\n",
    "        b_z = np.random.normal(loc=0.0, scale=1.0, size=(batch_size, z_dim)).astype(np.float32)\n",
    "        b_real_images = threading_data(b_real_images, prepro_img_256, mode='train')\n",
    "        b_wrong_images = threading_data(b_wrong_images, prepro_img_256, mode='train')\n",
    "        errRNN, _ = sess.run([model.rnn_loss_256, model.rnn_optim_256], feed_dict={\n",
    "                                        model.real_image_256 : b_real_images,\n",
    "                                        model.wrong_image_256 : b_wrong_images,\n",
    "                                        model.real_caption_256 : b_real_caption,\n",
    "                                        model.wrong_caption_256 : b_wrong_caption})\n",
    "        if (step + 1) % (n_batch_epoch // 10) == 0:\n",
    "            print(\"Step: [%d/%d] time: %4.4fs, Loss: %.8f\" % (step, n_batch_epoch, time.time() - step_time, errRNN))\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(\" ** Epoch %d took %fs\" % (epoch, time.time()-start_time))\n",
    "    if (epoch != 0) and (epoch % 20) == 0:\n",
    "        save_encoders_256(saver, sess, checkpoint_dir, epoch)\n",
    "        print(\"[*] Save checkpoints SUCCESS!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_r_precision_data():\n",
    "    caption_ids = np.reshape(np.asarray(test_dataset.captions_ids), (-1, cfg.TEXT.WORDS_NUM))\n",
    "    captions_ids_wrong = np.reshape(test_dataset.random_wrong_captions(), (-1, cfg.WRONG_CAPTION, cfg.TEXT.WORDS_NUM))\n",
    "\n",
    "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # load the trained checkpoint\n",
    "    checkpoint_dir = cfg.CHECKPOINT_DIR\n",
    "    if checkpoint_dir is not None:\n",
    "        loader = tf.train.Saver(var_list=tf.global_variables())\n",
    "        ckpt_path = os.path.join(cfg.CHECKPOINT_DIR, CHECKPOINT_NAME)\n",
    "        loader.restore(sess, ckpt_path)\n",
    "        print(\"Restored model parameters from {}\".format(ckpt_path))\n",
    "    else:\n",
    "        print('no checkpoints find.')\n",
    "\n",
    "    n_caption_test = len(caption_ids)\n",
    "    num_batches = n_caption_test // cfg.BATCH_SIZE\n",
    "\n",
    "    true_cnn_features = np.zeros((num_batches, cfg.BATCH_SIZE, cfg.TEXT.EMBEDDING_DIM), dtype=float)\n",
    "    true_rnn_features = np.zeros((num_batches, cfg.BATCH_SIZE, cfg.TEXT.EMBEDDING_DIM), dtype=float)\n",
    "    wrong_rnn_features = np.zeros((num_batches, cfg.WRONG_CAPTION, cfg.BATCH_SIZE, cfg.TEXT.EMBEDDING_DIM), dtype=float)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        test_cap = caption_ids[i * cfg.BATCH_SIZE: (i + 1) * cfg.BATCH_SIZE]\n",
    "\n",
    "        z = np.random.normal(loc=0.0, scale=1.0, size=(cfg.BATCH_SIZE, cfg.GAN.Z_DIM)).astype(np.float32)\n",
    "        \n",
    "        rnn_features = sess.run(rnn_encoder.outputs, feed_dict={t_real_caption: test_cap})\n",
    "        gen = sess.run(generator.outputs, feed_dict={t_real_caption: test_cap, t_z: z})\n",
    "        cnn_features = sess.run(cnn_encoder.outputs, feed_dict={t_real_image: gen})\n",
    "\n",
    "        true_cnn_features[i] = cnn_features\n",
    "        true_rnn_features[i] = rnn_features\n",
    "\n",
    "        for per_wrong_caption in range(cfg.WRONG_CAPTION):\n",
    "            test_cap = captions_ids_wrong[i * cfg.BATCH_SIZE: (i + 1) * cfg.BATCH_SIZE]\n",
    "            rnn_features = sess.run(rnn_encoder.outputs, feed_dict={t_real_caption: test_cap[:, per_wrong_caption]})\n",
    "            wrong_rnn_features[i, per_wrong_caption] = rnn_features\n",
    "    \n",
    "    # if exists, remove the existing file first\n",
    "    try:\n",
    "        os.remove(os.path.join(cfg.R_PRECISION_DIR, cfg.R_PRECISION_FILE))\n",
    "    except OSError:\n",
    "        pass\n",
    "    np.savez(os.path.join(cfg.R_PRECISION_DIR, cfg.R_PRECISION_FILE), true_cnn=true_cnn_features, true_rnn=true_rnn_features,\n",
    "             wrong_rnn=wrong_rnn_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inception_score_data():\n",
    "    caption_ids = np.reshape(np.asarray(test_dataset.captions_ids),\n",
    "                             (-1, cfg.TEXT.CAPTIONS_PER_IMAGE, cfg.TEXT.WORDS_NUM))\n",
    "    \n",
    "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    checkpoint_dir = cfg.CHECKPOINT_DIR\n",
    "    if checkpoint_dir is not None:\n",
    "        loader = tf.train.Saver(var_list=tf.global_variables())\n",
    "        ckpt_path = os.path.join(cfg.CHECKPOINT_DIR, cfg.CHECKPOINT_NAME)\n",
    "        loader.restore(sess, ckpt_path)\n",
    "        print(\"Restored model parameters from {}\".format(ckpt_path))\n",
    "    else:\n",
    "        print('no checkpoints find.')\n",
    "\n",
    "    n_caption_test = len(caption_ids)\n",
    "    num_batches = n_caption_test // cfg.BATCH_SIZE\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        for per_caption in range(cfg.TEXT.CAPTIONS_PER_IMAGE):\n",
    "            test_cap = caption_ids[i * cfg.BATCH_SIZE: (i + 1) * cfg.BATCH_SIZE, per_caption]\n",
    "            test_directory = test_dataset.filenames[i * cfg.BATCH_SIZE: (i + 1) * cfg.BATCH_SIZE]\n",
    "\n",
    "            z = np.random.normal(loc=0.0, scale=1.0, size=(cfg.BATCH_SIZE, cfg.GAN.Z_DIM)).astype(np.float32)\n",
    "            gen = sess.run(generator.outputs, feed_dict={t_real_caption: test_cap, t_z: z})\n",
    "            \n",
    "            for j in range(cfg.BATCH_SIZE):\n",
    "                if not os.path.exists(os.path.join(cfg.TEST.GENERATED_TEST_IMAGES, test_directory[j].split('/')[0])):\n",
    "                    os.mkdir(os.path.join(cfg.TEST.GENERATED_TEST_IMAGES, test_directory[j].split('/')[0]))\n",
    "\n",
    "                scipy.misc.imsave(os.path.join(cfg.TEST.GENERATED_TEST_IMAGES, test_directory[j] + '_{}.png'.format(per_caption)), gen[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_r_precision_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_inception_score_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Measure Inception score and R-precision of given test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After set the config file as 'eval_birds.yml' and run the 'generate_inception_score_data()' and 'generate_r_precision_data()', the synthesized images based on given captions and set of image and caption features should be saved inside a 'evaluation' folder, specifically in 'evaluation/generated_images/..' and as 'evaluation/r_precision.npz' respectively.\n",
    "\n",
    "**Then, go to the 'evaluation' folder and run each 'inception_score.ipynb' and 'r_precision.ipynb' file in order to measure inception score and r-precision score.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
