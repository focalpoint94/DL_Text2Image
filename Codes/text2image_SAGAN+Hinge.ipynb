{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br> Final Proejct: Text to Image Synthesis (Tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) Data Science & AI Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For understanding of this work, please carefully look at given PPT file.**\n",
    "\n",
    "**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the training process **</font> so that TAs can grade both your code and results.  \n",
    "**The TA will set a config file as 'eval_birds.yml' when evaluating the code using 'hidden test dataset'. Thus, please make sure that your code can generate proper data to measure inception score and R-precision of 'hidden test dataset'.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load datasets\n",
    "The Birds dataset will be downloaded automatically if it is not located in the *data* directory. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chszerg/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/chszerg/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/chszerg/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/chszerg/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/chszerg/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/chszerg/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os, nltk\n",
    "from miscc.config import cfg, cfg_from_file\n",
    "import pprint\n",
    "import datetime\n",
    "import dateutil.tz\n",
    "import numpy as np\n",
    "import scipy\n",
    "from utils.data_utils import CUBDataset\n",
    "from utils.loss import cosine_similarity\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "\n",
    "#################################################\n",
    "# DO NOT CHANGE \n",
    "from utils.model import CNN_ENCODER, RNN_ENCODER, GENERATOR, DISCRIMINATOR\n",
    "#################################################\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using config:\n",
      "{'BATCH_SIZE': 64,\n",
      " 'CHECKPOINT_DIR': './checkpoint',\n",
      " 'CHECKPOINT_NAME': 'model.ckpt',\n",
      " 'CNN': {'EMBEDDING_DIM': 0, 'H_DIM': 0},\n",
      " 'CONFIG_NAME': 'text-to-image',\n",
      " 'CUDA': False,\n",
      " 'DATASET_NAME': 'birds',\n",
      " 'DATA_DIR': 'data/birds',\n",
      " 'EMBEDDING_TYPE': 'cnn-rnn',\n",
      " 'GAN': {'B_ATTENTION': False,\n",
      "         'B_CONDITION': False,\n",
      "         'B_DCGAN': False,\n",
      "         'CONDITION_DIM': 0,\n",
      "         'DF_DIM': 0,\n",
      "         'EMBEDDING_DIM': 0,\n",
      "         'GF_DIM': 0,\n",
      "         'R_NUM': 0,\n",
      "         'Z_DIM': 512},\n",
      " 'GPU_ID': '0',\n",
      " 'IMAGE_SIZE': 256,\n",
      " 'NUM_BATCH_FOR_TEST': 0,\n",
      " 'RANDOM_SEED': 0,\n",
      " 'RNN': {'EMBEDDING_DIM': 0,\n",
      "         'H_DIM': 0,\n",
      "         'TYPE': '',\n",
      "         'VOCAB_SIZE': 0,\n",
      "         'WORD_EMBEDDING_DIM': 0},\n",
      " 'R_PRECISION_DIR': './evaluation',\n",
      " 'R_PRECISION_FILE': 'r_precision.npz',\n",
      " 'R_PRECISION_FILE_HIDDEN': 'r_precision_hidden.npz',\n",
      " 'TEST': {'B_EXAMPLE': False,\n",
      "          'GENERATED_HIDDEN_TEST_IMAGES': './evaluation/generated_images_hidden',\n",
      "          'GENERATED_TEST_IMAGES': './evaluation/generated_images'},\n",
      " 'TEXT': {'CAPTIONS_PER_IMAGE': 10, 'EMBEDDING_DIM': 128, 'WORDS_NUM': 20},\n",
      " 'TRAIN': {'CNN_ENCODER': '',\n",
      "           'COEFF': {'COLOR_LOSS': 0.0, 'KL': 0.0, 'UNCOND_LOSS': 0.0},\n",
      "           'DISCRIMINATOR': '',\n",
      "           'DISCRIMINATOR_LR': 0.0,\n",
      "           'FLAG': True,\n",
      "           'GENERATOR': '',\n",
      "           'GENERATOR_LR': 0.0,\n",
      "           'MAX_EPOCH': 600,\n",
      "           'RNN_ENCODER': '',\n",
      "           'SNAPSHOT_INTERVAL': 0},\n",
      " 'WORKERS': 4,\n",
      " 'WRONG_CAPTION': 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chszerg/final-project-deep-learning-19-tf/miscc/config.py:121: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  yaml_cfg = edict(yaml.load(f))\n"
     ]
    }
   ],
   "source": [
    "# Set a config file as 'train_birds.yml' in training, as 'eval_birds.yml' for evaluation\n",
    "cfg_from_file('cfg/train_birds.yml') # eval_birds.yml\n",
    "\n",
    "print('Using config:')\n",
    "pprint.pprint(cfg)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = cfg.GPU_ID\n",
    "\n",
    "now = datetime.datetime.now(dateutil.tz.tzlocal())\n",
    "timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
    "output_dir = 'sample/%s_%s_%s' % (cfg.DATASET_NAME, cfg.CONFIG_NAME, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.current_dir:\n",
      "/home/chszerg/final-project-deep-learning-19-tf\n",
      "\n",
      "self.data_dir:\n",
      "/home/chszerg/final-project-deep-learning-19-tf/data/birds\n",
      "\n",
      "self.image_dir:\n",
      "/home/chszerg/final-project-deep-learning-19-tf/data/birds/CUB_200_2011.tgz\n",
      "\n",
      "Dataset already exists\n",
      "self.image_dir:\n",
      "/home/chszerg/final-project-deep-learning-19-tf/data/birds/CUB_200_2011/images\n",
      "\n",
      "Load from:  data/birds/captions.pickle\n",
      "self.current_dir:\n",
      "/home/chszerg/final-project-deep-learning-19-tf\n",
      "\n",
      "self.data_dir:\n",
      "/home/chszerg/final-project-deep-learning-19-tf/data/birds\n",
      "\n",
      "self.image_dir:\n",
      "/home/chszerg/final-project-deep-learning-19-tf/data/birds/CUB_200_2011.tgz\n",
      "\n",
      "Dataset already exists\n",
      "self.image_dir:\n",
      "/home/chszerg/final-project-deep-learning-19-tf/data/birds/CUB_200_2011/images\n",
      "\n",
      "Load from:  data/birds/captions.pickle\n",
      "\n",
      "train data directory:\n",
      "/home/chszerg/final-project-deep-learning-19-tf/data/birds/train\n",
      "test data directory:\n",
      "/home/chszerg/final-project-deep-learning-19-tf/data/birds/test\n",
      "\n",
      "# of train filenames:(8855,)\n",
      "# of test filenames:(2933,)\n",
      "\n",
      "example of filename of train image:002.Laysan_Albatross/Laysan_Albatross_0002_1027\n",
      "example of filename of valid image:001.Black_footed_Albatross/Black_Footed_Albatross_0046_18\n",
      "\n",
      "example of caption and its ids:\n",
      "['a', 'bird', 'with', 'a', 'very', 'long', 'wing', 'span', 'and', 'a', 'long', 'pointed', 'beak']\n",
      "[ 1  2  3  1  4  5  6  7  8  1  5  9 10  0  0  0  0  0  0  0]\n",
      "\n",
      "example of caption and its ids:\n",
      "['light', 'tan', 'colored', 'bird', 'with', 'a', 'white', 'head', 'and', 'an', 'orange', 'beak']\n",
      "[ 67 106  89   2   3   1  14  25   8  28  52  10   0   0   0   0   0   0\n",
      "   0   0]\n",
      "\n",
      "# of train captions:(88550,)\n",
      "# of test captions:(29330,)\n",
      "\n",
      "# of train caption ids:(88550, 20)\n",
      "# of test caption ids:(29330, 20)\n",
      "\n",
      "# of train images:(8855, 256, 256, 3)\n",
      "# of test images:(2933, 256, 256, 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CUBDataset(cfg.DATA_DIR, split='train')\n",
    "test_dataset = CUBDataset(cfg.DATA_DIR, split='test')\n",
    "\n",
    "print(f'\\ntrain data directory:\\n{train_dataset.split_dir}')\n",
    "print(f'test data directory:\\n{test_dataset.split_dir}\\n')\n",
    "\n",
    "print(f'# of train filenames:{train_dataset.filenames.shape}')\n",
    "print(f'# of test filenames:{test_dataset.filenames.shape}\\n')\n",
    "\n",
    "print(f'example of filename of train image:{train_dataset.filenames[0]}')\n",
    "print(f'example of filename of valid image:{test_dataset.filenames[0]}\\n')\n",
    "\n",
    "print(f'example of caption and its ids:\\n{train_dataset.captions[0]}\\n{train_dataset.captions_ids[0]}\\n')\n",
    "print(f'example of caption and its ids:\\n{test_dataset.captions[0]}\\n{test_dataset.captions_ids[0]}\\n')\n",
    "\n",
    "print(f'# of train captions:{np.asarray(train_dataset.captions).shape}')\n",
    "print(f'# of test captions:{np.asarray(test_dataset.captions).shape}\\n')\n",
    "\n",
    "print(f'# of train caption ids:{np.asarray(train_dataset.captions_ids).shape}')\n",
    "print(f'# of test caption ids:{np.asarray(test_dataset.captions_ids).shape}\\n')\n",
    "\n",
    "print(f'# of train images:{train_dataset.images.shape}')\n",
    "print(f'# of test images:{test_dataset.images.shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8855, 256, 256, 3)\n",
      "(2933, 256, 256, 3)\n",
      "(88550, 20)\n",
      "(29330, 20)\n"
     ]
    }
   ],
   "source": [
    "train_images = train_dataset.images\n",
    "test_images = test_dataset.images\n",
    "train_captions = np.asarray(train_dataset.captions_ids)\n",
    "test_captions = np.asarray(test_dataset.captions_ids)\n",
    "print(train_images.shape)\n",
    "print(test_images.shape)\n",
    "print(train_captions.shape)\n",
    "print(test_captions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8855, 64, 64, 3)\n",
      "(2933, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "from skimage.transform import resize\n",
    "train_images_64 = []\n",
    "for train_image in train_images:\n",
    "    train_images_64.append(resize(train_image, (64, 64, 3)))\n",
    "train_images_64 = np.asarray(train_images_64)\n",
    "print(train_images_64.shape)\n",
    "assert train_images_64.shape[0] == train_images.shape[0]\n",
    "test_images_64 = []\n",
    "for test_image in test_images:\n",
    "    test_images_64.append(resize(test_image, (64, 64, 3)))\n",
    "test_images_64 = np.asarray(test_images_64)\n",
    "print(test_images_64.shape)\n",
    "assert test_images_64.shape[0] == test_images.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images_64\n",
    "test_images = test_images_64\n",
    "n_captions_train = len(train_captions)\n",
    "n_captions_per_image = 10\n",
    "n_images_train = len(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "import threading\n",
    "import scipy.ndimage as ndi\n",
    "from skimage import transform\n",
    "from skimage import exposure\n",
    "import skimage\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def sent2ID(sample_sentence):\n",
    "    caption = []\n",
    "    cap = sample_sentence\n",
    "    if len(cap) == 0:\n",
    "        exit()\n",
    "    cap = cap.replace(\"\\ufffd\\ufffd\", \" \")\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(cap.lower())\n",
    "    tokens_new = []\n",
    "    for t in tokens:\n",
    "        t = t.encode('ascii', 'ignore').decode('ascii')\n",
    "        if len(t) > 0:\n",
    "            tokens_new.append(t)\n",
    "    caption.append(tokens_new)\n",
    "    caption_new = []\n",
    "    t = caption[0]\n",
    "    rev = []\n",
    "    for w in t:\n",
    "        if w in train_dataset.wordtoix:\n",
    "            rev.append(train_dataset.wordtoix[w])\n",
    "    x, x_len = train_dataset.get_caption(rev)\n",
    "    caption_new.append(np.squeeze(x, axis=1))\n",
    "    return caption_new\n",
    "\n",
    "def ID2sent(sample_caption):\n",
    "    sentence = []\n",
    "    for ID in sample_caption:\n",
    "        if ID != train_dataset.ixtoword['<PAD>']:\n",
    "            sentence.append(train_dataset.ixtoword[ID])\n",
    "    return sentence\n",
    "\n",
    "def get_random_int(min=0, max=10, number=5):\n",
    "    return [random.randint(min,max) for p in range(0,number)]\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "    return img\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    return scipy.misc.imsave(path, merge(images, size))\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(images, size, image_path)\n",
    "\n",
    "def threading_data(data=None, fn=None, **kwargs):\n",
    "    def apply_fn(results, i, data, kwargs):\n",
    "        results[i] = fn(data, **kwargs)\n",
    "    results = [None] * len(data)\n",
    "    threads = []\n",
    "    for i in range(len(data)):\n",
    "        t = threading.Thread(\n",
    "                        name='threading_and_return',\n",
    "                        target=apply_fn,\n",
    "                        args=(results, i, data[i], kwargs)\n",
    "                        )\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    return np.asarray(results)\n",
    "\n",
    "def apply_transform(x, transform_matrix, channel_index=2, fill_mode='nearest', cval=0., order=1):\n",
    "    x = np.rollaxis(x, channel_index, 0)\n",
    "    final_affine_matrix = transform_matrix[:2, :2]\n",
    "    final_offset = transform_matrix[:2, 2]\n",
    "    channel_images = [ndi.interpolation.affine_transform(x_channel, final_affine_matrix,\n",
    "                      final_offset, order=order, mode=fill_mode, cval=cval) for x_channel in x]\n",
    "    x = np.stack(channel_images, axis=0)\n",
    "    x = np.rollaxis(x, 0, channel_index + 1)\n",
    "    return x\n",
    "\n",
    "def transform_matrix_offset_center(matrix, x, y):\n",
    "    o_x = float(x) / 2 + 0.5\n",
    "    o_y = float(y) / 2 + 0.5\n",
    "    offset_matrix = np.array([[1, 0, o_x], [0, 1, o_y], [0, 0, 1]])\n",
    "    reset_matrix = np.array([[1, 0, -o_x], [0, 1, -o_y], [0, 0, 1]])\n",
    "    transform_matrix = np.dot(np.dot(offset_matrix, matrix), reset_matrix)\n",
    "    return transform_matrix\n",
    "\n",
    "def rotation(x, rg=20, is_random=False, row_index=0, col_index=1, channel_index=2,\n",
    "                    fill_mode='nearest', cval=0.):\n",
    "    if is_random:\n",
    "        theta = np.pi / 180 * np.random.uniform(-rg, rg)\n",
    "    else:\n",
    "        theta = np.pi / 180 * rg\n",
    "    rotation_matrix = np.array([[np.cos(theta), -np.sin(theta), 0],\n",
    "                                [np.sin(theta), np.cos(theta), 0],\n",
    "                                [0, 0, 1]])\n",
    "    h, w = x.shape[row_index], x.shape[col_index]\n",
    "    transform_matrix = transform_matrix_offset_center(rotation_matrix, h, w)\n",
    "    x = apply_transform(x, transform_matrix, channel_index, fill_mode, cval)\n",
    "    return x\n",
    "\n",
    "def crop(x, wrg, hrg, is_random=False, row_index=0, col_index=1, channel_index=2):\n",
    "    h, w = x.shape[row_index], x.shape[col_index]\n",
    "    assert (h > hrg) and (w > wrg), \"The size of cropping should smaller than the original image\"\n",
    "    if is_random:\n",
    "        h_offset = int(np.random.uniform(0, h-hrg) - 1)\n",
    "        w_offset = int(np.random.uniform(0, w-wrg) - 1)\n",
    "        return x[h_offset: hrg + h_offset ,w_offset: wrg + w_offset]\n",
    "    else:\n",
    "        h_offset = int(np.floor((h - hrg)/ 2.))\n",
    "        w_offset = int(np.floor((w - wrg)/ 2.))\n",
    "        h_end = h_offset + hrg\n",
    "        w_end = w_offset + wrg\n",
    "        return x[h_offset: h_end, w_offset: w_end]\n",
    "\n",
    "def flip_axis(x, axis, is_random=False):\n",
    "    if is_random:\n",
    "        factor = np.random.uniform(-1, 1)\n",
    "        if factor > 0:\n",
    "            x = np.asarray(x).swapaxes(axis, 0)\n",
    "            x = x[::-1, ...]\n",
    "            x = x.swapaxes(0, axis)\n",
    "            return x\n",
    "        else:\n",
    "            return x\n",
    "    else:\n",
    "        x = np.asarray(x).swapaxes(axis, 0)\n",
    "        x = x[::-1, ...]\n",
    "        x = x.swapaxes(0, axis)\n",
    "        return x\n",
    "\n",
    "def imresize(x, size=[100, 100], interp='bilinear', mode=None):\n",
    "    if x.shape[-1] == 1:\n",
    "        x = scipy.misc.imresize(x[:, :, 0], size, interp=interp, mode=mode)\n",
    "        return x[:, :, np.newaxis]\n",
    "    elif x.shape[-1] == 3:\n",
    "        return scipy.misc.imresize(x, size, interp=interp, mode=mode)\n",
    "    else:\n",
    "        raise Exception(\"Unsupported channel %d\" % x.shape[-1])\n",
    "\n",
    "def prepro_img(x, mode=None):\n",
    "    if mode=='train':\n",
    "        x = flip_axis(x, axis=1, is_random=True)\n",
    "        x = rotation(x, rg=16, is_random=True, fill_mode='nearest')\n",
    "        x = imresize(x, size=[64 + 15, 64 + 15], interp='bilinear', mode=None)\n",
    "        x = crop(x, wrg=64, hrg=64, is_random=True)\n",
    "        x = x / (255. / 2.)\n",
    "        x = x - 1.\n",
    "    return x\n",
    "\n",
    "def combine_and_save_image_sets(image_sets, directory):\n",
    "    for i in range(len(image_sets[0])):\n",
    "        combined_image = []\n",
    "        for set_no in range(len(image_sets)):\n",
    "            combined_image.append(image_sets[set_no][i])\n",
    "            combined_image.append(np.zeros((image_sets[set_no][i].shape[0], 5, 3)))\n",
    "        combined_image = np.concatenate(combined_image, axis = 1)\n",
    "        scipy.misc.imsave(os.path.join(directory, 'combined_{}.jpg'.format(i)), combined_image)\n",
    "\n",
    "def save(saver, sess, logdir, step):\n",
    "    model_name = 'model.ckpt'\n",
    "    checkpoint_path = os.path.join(logdir, model_name)\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "    saver.save(sess, checkpoint_path, global_step=step)\n",
    "    print('The checkpoint has been created.')\n",
    "\n",
    "def load(saver, sess, ckpt_path):\n",
    "    saver.restore(sess, ckpt_path)\n",
    "    print(\"Restored model parameters from {}\".format(ckpt_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 20)\n"
     ]
    }
   ],
   "source": [
    "train_samples_dir = 'train_samples_last_gan'\n",
    "if os.path.exists(train_samples_dir) == False:\n",
    "    os.makedirs(train_samples_dir)\n",
    "checkpoint_dir = './checkpoint_last_gan'\n",
    "z_dim = 512\n",
    "image_size = 64\n",
    "c_dim = 3\n",
    "batch_size = 64\n",
    "ni = 8\n",
    "sample_size = batch_size\n",
    "sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, z_dim)).astype(np.float32)\n",
    "sample_sentence = [\"a black bird with oily black feathers and rounded black beak.\"] * int(sample_size/ni) + \\\n",
    "                  [\"a medium sized black bird, with a white belly, and webbed feet.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this is a white bird with black webbed feet and a black beak.\"] * int(sample_size/ni) + \\\n",
    "                  [\"a small dully colored bird that has a grey head and nape, an oatmeal colored breast, belly and yellow and oatmeal-grey colored wings and tail.\"] * int(sample_size/ni) + \\\n",
    "                  [\"a black bird with oily black feathers and rounded black beak.\"] * int(sample_size/ni) + \\\n",
    "                  [\"a medium sized black bird, with a white belly, and webbed feet.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this is a white bird with black webbed feet and a black beak.\"] * int(sample_size/ni) + \\\n",
    "                  [\"a small dully colored bird that has a grey head and nape, an oatmeal colored breast, belly and yellow and oatmeal-grey colored wings and tail.\"] * int(sample_size/ni)\n",
    "for i, sent in enumerate(sample_sentence):\n",
    "    sample_sentence[i] = sent2ID(sent)\n",
    "sample_sentence = np.asarray(sample_sentence)\n",
    "sample_sentence = np.reshape(sample_sentence, (sample_size, 20))\n",
    "print(sample_sentence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Text2Img:\n",
    "    def __init__(self):\n",
    "        \"\"\" Information \"\"\"\n",
    "        self.lr = 2e-4\n",
    "        self.z_dim = 512\n",
    "        self.image_size = 64\n",
    "        self.c_dim = 3\n",
    "        self.batch_size = 64\n",
    "        self.alpha = 0.2\n",
    "        \n",
    "        \"\"\" Place Holders \"\"\"\n",
    "        self.t_real_image = tf.placeholder('float32', [self.batch_size, self.image_size, image_size, 3], name = 'real_image')\n",
    "        self.t_wrong_image = tf.placeholder('float32', [self.batch_size ,self.image_size, image_size, 3], name = 'wrong_image')\n",
    "        self.t_real_caption = tf.placeholder(dtype=tf.int64, shape=[self.batch_size, None], name='real_caption_input')\n",
    "        self.t_wrong_caption = tf.placeholder(dtype=tf.int64, shape=[self.batch_size, None], name='wrong_caption_input')\n",
    "        self.t_z = tf.placeholder(tf.float32, [self.batch_size, self.z_dim], name='z_noise')\n",
    "        \n",
    "        \"\"\" Training Phase - CNN - RNN mapping \"\"\"\n",
    "        net_cnn = CNN_ENCODER(self.t_real_image, is_training=True, reuse=False)\n",
    "        x = net_cnn.outputs\n",
    "        v = RNN_ENCODER(self.t_real_caption, is_training=True, reuse=False).outputs\n",
    "        x_w = CNN_ENCODER(self.t_wrong_image, is_training=True, reuse=True).outputs\n",
    "        v_w = RNN_ENCODER(self.t_wrong_caption, is_training=True, reuse=True).outputs\n",
    "        self.rnn_loss = tf.reduce_mean(tf.maximum(0., self.alpha - cosine_similarity(x, v) + cosine_similarity(x, v_w))) + \\\n",
    "                    tf.reduce_mean(tf.maximum(0., self.alpha - cosine_similarity(x, v) + cosine_similarity(x_w, v)))\n",
    "        \n",
    "        \"\"\" Training Phase - GAN \"\"\"\n",
    "        self.net_rnn = RNN_ENCODER(self.t_real_caption, is_training=False, reuse=True)\n",
    "        net_fake_image = GENERATOR(self.t_z, self.net_rnn.outputs, is_training=True, reuse=False)\n",
    "        net_disc_fake = DISCRIMINATOR(net_fake_image.outputs, self.net_rnn.outputs, is_training=True, reuse=False)\n",
    "        disc_fake_logits = net_disc_fake.logits\n",
    "        net_disc_real = DISCRIMINATOR(self.t_real_image, self.net_rnn.outputs, is_training=True, reuse=True)\n",
    "        disc_real_logits = net_disc_real.logits\n",
    "        net_disc_mismatch = DISCRIMINATOR(self.t_real_image, RNN_ENCODER(self.t_wrong_caption, is_training=False, reuse=True).outputs,\n",
    "                                        is_training=True, reuse=True)\n",
    "        disc_mismatch_logits = net_disc_mismatch.logits\n",
    "        d_loss1 = tf.reduce_mean(tf.nn.relu(1.0 - disc_real_logits))\n",
    "        d_loss2 = tf.reduce_mean(tf.nn.relu(1.0 + disc_mismatch_logits))\n",
    "        d_loss3 = tf.reduce_mean(tf.nn.relu(1.0 + disc_fake_logits))\n",
    "        \"\"\"\n",
    "        d_loss1 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_real_logits,     labels=tf.ones_like(disc_real_logits),      name='d1'))\n",
    "        d_loss2 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_mismatch_logits, labels=tf.zeros_like(disc_mismatch_logits), name='d2'))\n",
    "        d_loss3 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_fake_logits,     labels=tf.zeros_like(disc_fake_logits),     name='d3'))\n",
    "        self.g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_fake_logits, labels=tf.ones_like(disc_fake_logits), name='g'))\n",
    "        \"\"\"\n",
    "        self.d_loss = d_loss1 + (d_loss2 + d_loss3) * 0.5\n",
    "        self.g_loss = -tf.reduce_mean(disc_fake_logits)\n",
    "        \n",
    "        \"\"\" Testing Phase \"\"\"\n",
    "        self.net_g = GENERATOR(self.t_z, RNN_ENCODER(self.t_real_caption, is_training=False, reuse=True).outputs,\n",
    "                            is_training=False, reuse=True)\n",
    "        \n",
    "        \"\"\" Training \"\"\"\n",
    "        rnn_vars = [var for var in tf.trainable_variables() if 'rnnencoder' in var.name]\n",
    "        cnn_vars = [var for var in tf.trainable_variables() if 'cnnencoder' in var.name]\n",
    "        d_vars = [var for var in tf.trainable_variables() if 'discriminator' in var.name]\n",
    "        g_vars = [var for var in tf.trainable_variables() if 'generator' in var.name]\n",
    "        update_ops_CNN = [var for var in tf.get_collection(tf.GraphKeys.UPDATE_OPS) if 'cnnencoder' in var.name]\n",
    "        update_ops_D = [var for var in tf.get_collection(tf.GraphKeys.UPDATE_OPS) if 'discriminator' in var.name]\n",
    "        update_ops_G = [var for var in tf.get_collection(tf.GraphKeys.UPDATE_OPS) if 'generator' in var.name]\n",
    "        with tf.variable_scope('learning_rate'):\n",
    "            self.lr_v = tf.Variable(self.lr, trainable=False)\n",
    "        with tf.control_dependencies(update_ops_CNN):\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(self.rnn_loss, rnn_vars + cnn_vars), 10)\n",
    "            optimizer = tf.train.AdamOptimizer(self.lr_v, beta1=0.5)\n",
    "            self.rnn_optim = optimizer.apply_gradients(zip(grads, rnn_vars + cnn_vars))\n",
    "        with tf.control_dependencies(update_ops_D):\n",
    "            self.d_optim = tf.train.AdamOptimizer(2e-4, beta1=0.0, beta2=0.9).minimize(self.d_loss, var_list=d_vars)\n",
    "        with tf.control_dependencies(update_ops_G):\n",
    "            self.g_optim = tf.train.AdamOptimizer(2e-4, beta1=0.0, beta2=0.9).minimize(self.g_loss, var_list=g_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/chszerg/final-project-deep-learning-19-tf/utils/model.py:368: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "no checkpoints find.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chszerg/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/ipykernel_launcher.py:142: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/2000] time: 0.6448s, d_loss: 2.41165090, g_loss: -0.52209276, rnn_loss: 0.29912937\n",
      " ** Epoch 0 took 107.678850s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chszerg/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/ipykernel_launcher.py:53: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/2000] time: 0.6398s, d_loss: 1.82504320, g_loss: 0.91173375, rnn_loss: 0.27283305\n",
      " ** Epoch 1 took 87.974666s\n",
      "Epoch: [2/2000] time: 0.6528s, d_loss: 1.97624636, g_loss: 0.79536945, rnn_loss: 0.29435828\n",
      " ** Epoch 2 took 89.082275s\n",
      "Epoch: [3/2000] time: 0.6424s, d_loss: 1.90790009, g_loss: 0.36982119, rnn_loss: 0.29710805\n",
      " ** Epoch 3 took 88.648509s\n",
      "Epoch: [4/2000] time: 0.6362s, d_loss: 1.95481372, g_loss: 0.23233420, rnn_loss: 0.22478952\n",
      " ** Epoch 4 took 88.208620s\n",
      "Epoch: [5/2000] time: 0.6499s, d_loss: 1.90160251, g_loss: 0.23809411, rnn_loss: 0.21761549\n",
      " ** Epoch 5 took 87.960535s\n",
      "Epoch: [6/2000] time: 0.6494s, d_loss: 1.78657043, g_loss: 0.30586457, rnn_loss: 0.21422032\n",
      " ** Epoch 6 took 88.150791s\n",
      "Epoch: [7/2000] time: 0.6319s, d_loss: 2.15114570, g_loss: -0.17194062, rnn_loss: 0.25098729\n",
      " ** Epoch 7 took 88.000345s\n",
      "Epoch: [8/2000] time: 0.6380s, d_loss: 1.58100545, g_loss: -1.11637771, rnn_loss: 0.18536358\n",
      " ** Epoch 8 took 87.879308s\n",
      "Epoch: [9/2000] time: 0.6336s, d_loss: 2.17711377, g_loss: -0.70424795, rnn_loss: 0.19502477\n",
      " ** Epoch 9 took 88.269133s\n",
      "Epoch: [10/2000] time: 0.6471s, d_loss: 1.56711340, g_loss: 0.38312072, rnn_loss: 0.16692060\n",
      " ** Epoch 10 took 88.107302s\n",
      "Epoch: [11/2000] time: 0.6357s, d_loss: 1.49194288, g_loss: 0.09697692, rnn_loss: 0.18233281\n",
      " ** Epoch 11 took 88.125736s\n",
      "Epoch: [12/2000] time: 0.6363s, d_loss: 1.68515396, g_loss: -0.32917452, rnn_loss: 0.18795930\n",
      " ** Epoch 12 took 88.139390s\n",
      "Epoch: [13/2000] time: 0.6332s, d_loss: 1.36969352, g_loss: 0.09152874, rnn_loss: 0.19576712\n",
      " ** Epoch 13 took 88.519170s\n",
      "Epoch: [14/2000] time: 0.6411s, d_loss: 1.35974145, g_loss: -0.47145611, rnn_loss: 0.19939661\n",
      " ** Epoch 14 took 88.075227s\n",
      "Epoch: [15/2000] time: 0.6342s, d_loss: 1.64569688, g_loss: -0.41399491, rnn_loss: 0.20259857\n",
      " ** Epoch 15 took 87.962687s\n",
      "Epoch: [16/2000] time: 0.6535s, d_loss: 1.65259361, g_loss: -0.13245028, rnn_loss: 0.18514213\n",
      " ** Epoch 16 took 88.518385s\n",
      "Epoch: [17/2000] time: 0.6364s, d_loss: 1.10801995, g_loss: -0.03299576, rnn_loss: 0.18271425\n",
      " ** Epoch 17 took 88.038484s\n",
      "Epoch: [18/2000] time: 0.6447s, d_loss: 1.25850344, g_loss: -1.03979397, rnn_loss: 0.21114604\n",
      " ** Epoch 18 took 87.974350s\n",
      "Epoch: [19/2000] time: 0.6365s, d_loss: 1.56425583, g_loss: -0.93007922, rnn_loss: 0.23735991\n",
      " ** Epoch 19 took 87.903387s\n",
      "Epoch: [20/2000] time: 0.6407s, d_loss: 1.55773628, g_loss: -0.55888015, rnn_loss: 0.15872726\n",
      " ** Epoch 20 took 88.302821s\n",
      "Epoch: [21/2000] time: 0.6366s, d_loss: 1.38326442, g_loss: -0.46214822, rnn_loss: 0.19919546\n",
      " ** Epoch 21 took 87.818898s\n",
      "Epoch: [22/2000] time: 0.6325s, d_loss: 1.67833304, g_loss: -0.75676870, rnn_loss: 0.25054935\n",
      " ** Epoch 22 took 87.870375s\n",
      "Epoch: [23/2000] time: 0.6567s, d_loss: 1.73093414, g_loss: -0.62277007, rnn_loss: 0.19100086\n",
      " ** Epoch 23 took 87.992064s\n",
      "Epoch: [24/2000] time: 0.6429s, d_loss: 1.54672527, g_loss: 0.57935917, rnn_loss: 0.16620946\n",
      " ** Epoch 24 took 88.090318s\n",
      "Epoch: [25/2000] time: 0.6402s, d_loss: 1.41642070, g_loss: -1.07293141, rnn_loss: 0.18283314\n",
      " ** Epoch 25 took 87.998787s\n",
      "Epoch: [26/2000] time: 0.6345s, d_loss: 1.06986701, g_loss: -0.67041075, rnn_loss: 0.19798028\n",
      " ** Epoch 26 took 88.187509s\n",
      "Epoch: [27/2000] time: 0.6398s, d_loss: 1.14978933, g_loss: -0.60205501, rnn_loss: 0.13277924\n",
      " ** Epoch 27 took 88.693275s\n",
      "Epoch: [28/2000] time: 0.6395s, d_loss: 1.18137991, g_loss: -0.87371743, rnn_loss: 0.18352708\n",
      " ** Epoch 28 took 88.575081s\n",
      "Epoch: [29/2000] time: 0.6415s, d_loss: 1.50797296, g_loss: -0.56204945, rnn_loss: 0.17181653\n",
      " ** Epoch 29 took 89.253220s\n",
      "Epoch: [30/2000] time: 0.6526s, d_loss: 1.33404028, g_loss: -0.87487912, rnn_loss: 0.18739837\n",
      " ** Epoch 30 took 88.454303s\n",
      "Epoch: [31/2000] time: 0.6393s, d_loss: 1.53528738, g_loss: -1.08682871, rnn_loss: 0.15309615\n",
      " ** Epoch 31 took 88.307001s\n",
      "Epoch: [32/2000] time: 0.6356s, d_loss: 1.82895243, g_loss: 0.38878992, rnn_loss: 0.16536646\n",
      " ** Epoch 32 took 88.219233s\n",
      "Epoch: [33/2000] time: 0.6510s, d_loss: 1.33764732, g_loss: -0.89221215, rnn_loss: 0.21413293\n",
      " ** Epoch 33 took 88.207677s\n",
      "Epoch: [34/2000] time: 0.6394s, d_loss: 1.12853205, g_loss: -0.63348830, rnn_loss: 0.13133717\n",
      " ** Epoch 34 took 88.471392s\n",
      "Epoch: [35/2000] time: 0.6331s, d_loss: 1.16503417, g_loss: -1.06108308, rnn_loss: 0.17205818\n",
      " ** Epoch 35 took 87.794854s\n",
      "Epoch: [36/2000] time: 0.6329s, d_loss: 0.92935276, g_loss: 0.27776748, rnn_loss: 0.14052518\n",
      " ** Epoch 36 took 87.894024s\n",
      "Epoch: [37/2000] time: 0.6535s, d_loss: 1.43318522, g_loss: -0.43326843, rnn_loss: 0.14501789\n",
      " ** Epoch 37 took 88.078646s\n",
      "Epoch: [38/2000] time: 0.6345s, d_loss: 1.17336774, g_loss: -0.63640273, rnn_loss: 0.15504210\n",
      " ** Epoch 38 took 88.383339s\n",
      "Epoch: [39/2000] time: 0.6379s, d_loss: 1.28651869, g_loss: 0.18755588, rnn_loss: 0.15967503\n",
      " ** Epoch 39 took 87.915401s\n",
      "Epoch: [40/2000] time: 0.6342s, d_loss: 1.20831597, g_loss: 0.77478957, rnn_loss: 0.14936161\n",
      " ** Epoch 40 took 87.862304s\n",
      "Epoch: [41/2000] time: 0.6416s, d_loss: 1.18902564, g_loss: -0.56857109, rnn_loss: 0.18141082\n",
      " ** Epoch 41 took 88.262129s\n",
      "Epoch: [42/2000] time: 0.6361s, d_loss: 1.31694293, g_loss: 0.87039572, rnn_loss: 0.14960656\n",
      " ** Epoch 42 took 87.986848s\n",
      "Epoch: [43/2000] time: 0.6398s, d_loss: 1.04231071, g_loss: 0.05299503, rnn_loss: 0.16404532\n",
      " ** Epoch 43 took 88.094168s\n",
      "Epoch: [44/2000] time: 0.6483s, d_loss: 1.06127954, g_loss: -0.79282522, rnn_loss: 0.12734109\n",
      " ** Epoch 44 took 88.302812s\n",
      "Epoch: [45/2000] time: 0.6387s, d_loss: 1.18233061, g_loss: -0.81584901, rnn_loss: 0.08991019\n",
      " ** Epoch 45 took 88.322879s\n",
      "Epoch: [46/2000] time: 0.6336s, d_loss: 1.01989806, g_loss: -0.15763499, rnn_loss: 0.14439817\n",
      " ** Epoch 46 took 87.756497s\n",
      "Epoch: [47/2000] time: 0.6341s, d_loss: 1.12987447, g_loss: -0.66258514, rnn_loss: 0.16531101\n",
      " ** Epoch 47 took 87.680352s\n",
      "Epoch: [48/2000] time: 0.6322s, d_loss: 1.24414611, g_loss: -0.42422521, rnn_loss: 0.16105849\n",
      " ** Epoch 48 took 88.009581s\n",
      "Epoch: [49/2000] time: 0.6349s, d_loss: 0.87055963, g_loss: 0.63348967, rnn_loss: 0.17098384\n",
      " ** Epoch 49 took 87.689150s\n",
      "Epoch: [50/2000] time: 0.6381s, d_loss: 0.87016702, g_loss: 1.06249964, rnn_loss: 0.18362728\n",
      " ** Epoch 50 took 87.650077s\n",
      "The checkpoint has been created.\n",
      "[*] Save checkpoints SUCCESS!\n",
      "Epoch: [51/2000] time: 0.6318s, d_loss: 1.74370468, g_loss: 0.37166199, rnn_loss: 0.16927896\n",
      " ** Epoch 51 took 87.656457s\n",
      "Epoch: [52/2000] time: 0.6350s, d_loss: 1.17679262, g_loss: 0.22246653, rnn_loss: 0.13724664\n",
      " ** Epoch 52 took 88.007645s\n",
      "Epoch: [53/2000] time: 0.6310s, d_loss: 1.10247183, g_loss: -0.36540851, rnn_loss: 0.12897545\n",
      " ** Epoch 53 took 87.457669s\n",
      "Epoch: [54/2000] time: 0.6452s, d_loss: 0.76693487, g_loss: 0.97716361, rnn_loss: 0.13480765\n",
      " ** Epoch 54 took 87.697454s\n",
      "Epoch: [55/2000] time: 0.6449s, d_loss: 1.10813439, g_loss: -0.12363937, rnn_loss: 0.16205874\n",
      " ** Epoch 55 took 87.867588s\n",
      "Epoch: [56/2000] time: 0.6275s, d_loss: 0.89141703, g_loss: -0.02937248, rnn_loss: 0.16687848\n",
      " ** Epoch 56 took 87.271944s\n",
      "Epoch: [57/2000] time: 0.6377s, d_loss: 0.87175369, g_loss: 0.20528919, rnn_loss: 0.15401199\n",
      " ** Epoch 57 took 87.292124s\n",
      "Epoch: [58/2000] time: 0.6336s, d_loss: 1.06747806, g_loss: 0.96254915, rnn_loss: 0.13526654\n",
      " ** Epoch 58 took 87.432821s\n",
      "Epoch: [59/2000] time: 0.6310s, d_loss: 0.93643153, g_loss: 0.43544233, rnn_loss: 0.17724392\n",
      " ** Epoch 59 took 87.502479s\n",
      "Epoch: [60/2000] time: 0.6341s, d_loss: 1.05366051, g_loss: -0.51103294, rnn_loss: 0.18936560\n",
      " ** Epoch 60 took 87.226001s\n",
      "Epoch: [61/2000] time: 0.6299s, d_loss: 1.15912592, g_loss: -0.37100357, rnn_loss: 0.14708540\n",
      " ** Epoch 61 took 87.234005s\n",
      "Epoch: [62/2000] time: 0.6422s, d_loss: 0.99534261, g_loss: 0.56720185, rnn_loss: 0.13805181\n",
      " ** Epoch 62 took 87.872715s\n",
      "Epoch: [63/2000] time: 0.6290s, d_loss: 1.05766344, g_loss: 0.62487924, rnn_loss: 0.16626814\n",
      " ** Epoch 63 took 87.240219s\n",
      "Epoch: [64/2000] time: 0.6278s, d_loss: 1.05255842, g_loss: -0.34900081, rnn_loss: 0.12509269\n",
      " ** Epoch 64 took 87.156878s\n",
      "Epoch: [65/2000] time: 0.6324s, d_loss: 0.80366266, g_loss: 0.89452744, rnn_loss: 0.11836938\n",
      " ** Epoch 65 took 87.283207s\n",
      "Epoch: [66/2000] time: 0.6308s, d_loss: 0.82934195, g_loss: 0.34805083, rnn_loss: 0.15896758\n",
      " ** Epoch 66 took 87.505282s\n",
      "Epoch: [67/2000] time: 0.6357s, d_loss: 1.72297406, g_loss: -0.28127992, rnn_loss: 0.13466319\n",
      " ** Epoch 67 took 87.123886s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [68/2000] time: 0.6318s, d_loss: 0.93331707, g_loss: 1.12794232, rnn_loss: 0.16681527\n",
      " ** Epoch 68 took 87.164877s\n",
      "Epoch: [69/2000] time: 0.6296s, d_loss: 1.10689425, g_loss: -0.01933331, rnn_loss: 0.13701554\n",
      " ** Epoch 69 took 87.621413s\n",
      "Epoch: [70/2000] time: 0.6292s, d_loss: 1.65472376, g_loss: -0.17475992, rnn_loss: 0.15100332\n",
      " ** Epoch 70 took 87.144378s\n",
      "Epoch: [71/2000] time: 0.6332s, d_loss: 0.92351770, g_loss: 0.34861565, rnn_loss: 0.13049668\n",
      " ** Epoch 71 took 87.138511s\n",
      "Epoch: [72/2000] time: 0.6297s, d_loss: 1.03632307, g_loss: -0.38667688, rnn_loss: 0.15453096\n",
      " ** Epoch 72 took 87.228329s\n",
      "Epoch: [73/2000] time: 0.6331s, d_loss: 0.89032412, g_loss: -0.05895571, rnn_loss: 0.10791834\n",
      " ** Epoch 73 took 87.732697s\n",
      "Epoch: [74/2000] time: 0.6383s, d_loss: 0.92143804, g_loss: 1.35954428, rnn_loss: 0.13127509\n",
      " ** Epoch 74 took 87.344467s\n",
      "Epoch: [75/2000] time: 0.6311s, d_loss: 1.08213806, g_loss: 0.95438057, rnn_loss: 0.11386327\n",
      " ** Epoch 75 took 87.400697s\n",
      "Epoch: [76/2000] time: 0.6336s, d_loss: 1.18306327, g_loss: -0.53082818, rnn_loss: 0.11858936\n",
      " ** Epoch 76 took 87.932112s\n",
      "Epoch: [77/2000] time: 0.6318s, d_loss: 0.89623421, g_loss: 1.25648916, rnn_loss: 0.10465881\n",
      " ** Epoch 77 took 87.525773s\n",
      "Epoch: [78/2000] time: 0.6341s, d_loss: 0.99491107, g_loss: 0.74749553, rnn_loss: 0.16297200\n",
      " ** Epoch 78 took 87.537615s\n",
      "Epoch: [79/2000] time: 0.6354s, d_loss: 1.00233006, g_loss: 0.52275670, rnn_loss: 0.19240567\n",
      " ** Epoch 79 took 88.357321s\n",
      "Epoch: [80/2000] time: 0.6366s, d_loss: 1.19783413, g_loss: -0.31186572, rnn_loss: 0.16073138\n",
      " ** Epoch 80 took 88.291008s\n",
      "Epoch: [81/2000] time: 0.6337s, d_loss: 0.84153092, g_loss: -0.17247346, rnn_loss: 0.14061862\n",
      " ** Epoch 81 took 87.508146s\n",
      "Epoch: [82/2000] time: 0.6341s, d_loss: 1.05004168, g_loss: 0.48551747, rnn_loss: 0.17574653\n",
      " ** Epoch 82 took 87.410760s\n",
      "Epoch: [83/2000] time: 0.6518s, d_loss: 1.10592532, g_loss: 0.05151796, rnn_loss: 0.11201042\n",
      " ** Epoch 83 took 87.789570s\n",
      "Epoch: [84/2000] time: 0.6340s, d_loss: 0.89106327, g_loss: 0.22482285, rnn_loss: 0.09944806\n",
      " ** Epoch 84 took 88.288625s\n",
      "Epoch: [85/2000] time: 0.6331s, d_loss: 0.70120674, g_loss: -0.05273153, rnn_loss: 0.15655953\n",
      " ** Epoch 85 took 87.545907s\n",
      "Epoch: [86/2000] time: 0.6320s, d_loss: 0.62830520, g_loss: 0.61469561, rnn_loss: 0.07607878\n",
      " ** Epoch 86 took 87.747949s\n",
      "Epoch: [87/2000] time: 0.6344s, d_loss: 1.03292274, g_loss: 1.03461635, rnn_loss: 0.13220790\n",
      " ** Epoch 87 took 87.719347s\n",
      "Epoch: [88/2000] time: 0.6331s, d_loss: 1.11677122, g_loss: -0.30252835, rnn_loss: 0.11272469\n",
      " ** Epoch 88 took 87.292686s\n",
      "Epoch: [89/2000] time: 0.6300s, d_loss: 0.91644394, g_loss: 0.08845617, rnn_loss: 0.12420619\n",
      " ** Epoch 89 took 87.258886s\n",
      "Epoch: [90/2000] time: 0.6468s, d_loss: 0.73459572, g_loss: -0.14220950, rnn_loss: 0.17956431\n",
      " ** Epoch 90 took 87.506313s\n",
      "Epoch: [91/2000] time: 0.6337s, d_loss: 0.88422525, g_loss: 1.93817759, rnn_loss: 0.13211136\n",
      " ** Epoch 91 took 87.532087s\n",
      "Epoch: [92/2000] time: 0.6298s, d_loss: 1.13202345, g_loss: 1.47917926, rnn_loss: 0.13651562\n",
      " ** Epoch 92 took 87.328560s\n",
      "Epoch: [93/2000] time: 0.6317s, d_loss: 0.85107529, g_loss: 0.20727995, rnn_loss: 0.14162600\n",
      " ** Epoch 93 took 87.480874s\n",
      "Epoch: [94/2000] time: 0.6327s, d_loss: 0.75565147, g_loss: 1.47503471, rnn_loss: 0.14036751\n",
      " ** Epoch 94 took 87.641996s\n",
      "Epoch: [95/2000] time: 0.6298s, d_loss: 1.53809679, g_loss: -0.37625426, rnn_loss: 0.11588255\n",
      " ** Epoch 95 took 87.366529s\n",
      "Epoch: [96/2000] time: 0.6312s, d_loss: 0.86019659, g_loss: 1.01224208, rnn_loss: 0.14264701\n",
      " ** Epoch 96 took 87.904559s\n",
      "Epoch: [97/2000] time: 0.6411s, d_loss: 0.98915780, g_loss: 1.05280161, rnn_loss: 0.13737610\n",
      " ** Epoch 97 took 87.776286s\n",
      "Epoch: [98/2000] time: 0.6379s, d_loss: 0.93635690, g_loss: 0.94660926, rnn_loss: 0.12110800\n",
      " ** Epoch 98 took 87.736524s\n",
      "Epoch: [99/2000] time: 0.6345s, d_loss: 0.90686393, g_loss: 1.40098941, rnn_loss: 0.12820300\n",
      " ** Epoch 99 took 88.160527s\n",
      "Epoch: [100/2000] time: 0.5881s, d_loss: 0.76164156, g_loss: 1.33214736, rnn_loss: 0.00000000\n",
      " ** Epoch 100 took 81.816704s\n",
      "The checkpoint has been created.\n",
      "[*] Save checkpoints SUCCESS!\n",
      "Epoch: [101/2000] time: 0.5903s, d_loss: 0.97781718, g_loss: 1.22679591, rnn_loss: 0.00000000\n",
      " ** Epoch 101 took 81.837366s\n",
      "Epoch: [102/2000] time: 0.5897s, d_loss: 1.18564129, g_loss: -0.20792092, rnn_loss: 0.00000000\n",
      " ** Epoch 102 took 81.834836s\n",
      "Epoch: [103/2000] time: 0.5965s, d_loss: 0.95211411, g_loss: -0.34124458, rnn_loss: 0.00000000\n",
      " ** Epoch 103 took 81.859207s\n",
      "Epoch: [104/2000] time: 0.5915s, d_loss: 1.00451517, g_loss: 2.02376485, rnn_loss: 0.00000000\n",
      " ** Epoch 104 took 81.911191s\n",
      "Epoch: [105/2000] time: 0.5947s, d_loss: 0.82159197, g_loss: 1.70948207, rnn_loss: 0.00000000\n",
      " ** Epoch 105 took 82.061733s\n",
      "Epoch: [106/2000] time: 0.5913s, d_loss: 0.78567457, g_loss: 0.17423917, rnn_loss: 0.00000000\n",
      " ** Epoch 106 took 81.833188s\n",
      "Epoch: [107/2000] time: 0.5923s, d_loss: 0.65549898, g_loss: 1.42559814, rnn_loss: 0.00000000\n",
      " ** Epoch 107 took 81.747937s\n",
      "Epoch: [108/2000] time: 0.5983s, d_loss: 0.86666083, g_loss: -0.04533064, rnn_loss: 0.00000000\n",
      " ** Epoch 108 took 81.841048s\n",
      "Epoch: [109/2000] time: 0.5977s, d_loss: 1.08872581, g_loss: 1.09775996, rnn_loss: 0.00000000\n",
      " ** Epoch 109 took 82.216294s\n",
      "Epoch: [110/2000] time: 0.5921s, d_loss: 1.36339724, g_loss: -0.36691046, rnn_loss: 0.00000000\n",
      " ** Epoch 110 took 81.556204s\n",
      "Epoch: [111/2000] time: 0.5888s, d_loss: 0.85589153, g_loss: 1.70284808, rnn_loss: 0.00000000\n",
      " ** Epoch 111 took 82.585086s\n",
      "Epoch: [112/2000] time: 0.6014s, d_loss: 1.06062162, g_loss: -0.19285041, rnn_loss: 0.00000000\n",
      " ** Epoch 112 took 81.906573s\n",
      "Epoch: [113/2000] time: 0.5866s, d_loss: 0.99834973, g_loss: 0.21675423, rnn_loss: 0.00000000\n",
      " ** Epoch 113 took 81.936263s\n",
      "Epoch: [114/2000] time: 0.5897s, d_loss: 0.77134877, g_loss: 1.63215470, rnn_loss: 0.00000000\n",
      " ** Epoch 114 took 81.638191s\n",
      "Epoch: [115/2000] time: 0.5917s, d_loss: 0.86159056, g_loss: 1.73012304, rnn_loss: 0.00000000\n",
      " ** Epoch 115 took 81.690045s\n",
      "Epoch: [116/2000] time: 0.5884s, d_loss: 1.11809039, g_loss: -0.49716136, rnn_loss: 0.00000000\n",
      " ** Epoch 116 took 81.921839s\n",
      "Epoch: [117/2000] time: 0.5940s, d_loss: 0.62561345, g_loss: 1.64049566, rnn_loss: 0.00000000\n",
      " ** Epoch 117 took 81.608598s\n",
      "Epoch: [118/2000] time: 0.5898s, d_loss: 0.85973430, g_loss: 0.43932617, rnn_loss: 0.00000000\n",
      " ** Epoch 118 took 81.569013s\n",
      "Epoch: [119/2000] time: 0.5916s, d_loss: 0.75139439, g_loss: -0.32709146, rnn_loss: 0.00000000\n",
      " ** Epoch 119 took 81.703791s\n",
      "Epoch: [120/2000] time: 0.5908s, d_loss: 0.62371957, g_loss: 0.37857449, rnn_loss: 0.00000000\n",
      " ** Epoch 120 took 81.830673s\n",
      "Epoch: [121/2000] time: 0.5894s, d_loss: 0.87454700, g_loss: -0.27511597, rnn_loss: 0.00000000\n",
      " ** Epoch 121 took 81.546591s\n",
      "Epoch: [122/2000] time: 0.5968s, d_loss: 0.42989767, g_loss: 2.22920561, rnn_loss: 0.00000000\n",
      " ** Epoch 122 took 81.584320s\n",
      "Epoch: [123/2000] time: 0.5942s, d_loss: 0.94849938, g_loss: 1.35779929, rnn_loss: 0.00000000\n",
      " ** Epoch 123 took 81.554299s\n",
      "Epoch: [124/2000] time: 0.5861s, d_loss: 0.78514707, g_loss: 1.70468116, rnn_loss: 0.00000000\n",
      " ** Epoch 124 took 81.757830s\n",
      "Epoch: [125/2000] time: 0.5902s, d_loss: 1.13219345, g_loss: -0.11500267, rnn_loss: 0.00000000\n",
      " ** Epoch 125 took 81.448637s\n",
      "Epoch: [126/2000] time: 0.5989s, d_loss: 0.83190131, g_loss: -0.19315422, rnn_loss: 0.00000000\n",
      " ** Epoch 126 took 81.619731s\n",
      "Epoch: [127/2000] time: 0.5859s, d_loss: 0.73257148, g_loss: 2.77791214, rnn_loss: 0.00000000\n",
      " ** Epoch 127 took 81.547728s\n",
      "Epoch: [128/2000] time: 0.5868s, d_loss: 0.79718506, g_loss: 2.07734823, rnn_loss: 0.00000000\n",
      " ** Epoch 128 took 81.740560s\n",
      "Epoch: [129/2000] time: 0.5863s, d_loss: 1.15083539, g_loss: 1.63856769, rnn_loss: 0.00000000\n",
      " ** Epoch 129 took 81.495595s\n",
      "Epoch: [130/2000] time: 0.5896s, d_loss: 1.06611180, g_loss: 0.10249941, rnn_loss: 0.00000000\n",
      " ** Epoch 130 took 81.622299s\n",
      "Epoch: [131/2000] time: 0.6071s, d_loss: 0.53207231, g_loss: 1.67515659, rnn_loss: 0.00000000\n",
      " ** Epoch 131 took 81.586467s\n",
      "Epoch: [132/2000] time: 0.5885s, d_loss: 0.84330010, g_loss: 0.90830898, rnn_loss: 0.00000000\n",
      " ** Epoch 132 took 81.754608s\n",
      "Epoch: [133/2000] time: 0.5871s, d_loss: 1.02976680, g_loss: -0.26682559, rnn_loss: 0.00000000\n",
      " ** Epoch 133 took 81.504763s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [134/2000] time: 0.5898s, d_loss: 1.74463665, g_loss: 0.06658374, rnn_loss: 0.00000000\n",
      " ** Epoch 134 took 81.641809s\n",
      "Epoch: [135/2000] time: 0.5904s, d_loss: 1.10826671, g_loss: 1.79035294, rnn_loss: 0.00000000\n",
      " ** Epoch 135 took 81.758289s\n",
      "Epoch: [136/2000] time: 0.5904s, d_loss: 0.82929718, g_loss: 1.83862329, rnn_loss: 0.00000000\n",
      " ** Epoch 136 took 81.536198s\n",
      "Epoch: [137/2000] time: 0.5903s, d_loss: 0.97374743, g_loss: 1.68270350, rnn_loss: 0.00000000\n",
      " ** Epoch 137 took 81.662028s\n",
      "Epoch: [138/2000] time: 0.5935s, d_loss: 0.47249654, g_loss: 0.47300038, rnn_loss: 0.00000000\n",
      " ** Epoch 138 took 81.509124s\n",
      "Epoch: [139/2000] time: 0.5934s, d_loss: 0.83905131, g_loss: 1.81350148, rnn_loss: 0.00000000\n",
      " ** Epoch 139 took 81.787178s\n",
      "Epoch: [140/2000] time: 0.5893s, d_loss: 0.36184788, g_loss: 0.69829857, rnn_loss: 0.00000000\n",
      " ** Epoch 140 took 81.413826s\n",
      "Epoch: [141/2000] time: 0.5906s, d_loss: 0.62442529, g_loss: 0.61822486, rnn_loss: 0.00000000\n",
      " ** Epoch 141 took 81.735027s\n",
      "Epoch: [142/2000] time: 0.5899s, d_loss: 0.47899410, g_loss: 1.13093305, rnn_loss: 0.00000000\n",
      " ** Epoch 142 took 81.415822s\n",
      "Epoch: [143/2000] time: 0.5925s, d_loss: 0.86700422, g_loss: 2.79291964, rnn_loss: 0.00000000\n",
      " ** Epoch 143 took 81.725919s\n",
      "Epoch: [144/2000] time: 0.5906s, d_loss: 0.61544859, g_loss: 1.12222278, rnn_loss: 0.00000000\n",
      " ** Epoch 144 took 81.430805s\n",
      "Epoch: [145/2000] time: 0.5944s, d_loss: 0.56852478, g_loss: -0.15794076, rnn_loss: 0.00000000\n",
      " ** Epoch 145 took 81.532213s\n",
      "Epoch: [146/2000] time: 0.5870s, d_loss: 0.47837102, g_loss: 0.58473873, rnn_loss: 0.00000000\n",
      " ** Epoch 146 took 81.446921s\n",
      "Epoch: [147/2000] time: 0.5883s, d_loss: 0.36819461, g_loss: 3.00194025, rnn_loss: 0.00000000\n",
      " ** Epoch 147 took 81.755689s\n",
      "Epoch: [148/2000] time: 0.5941s, d_loss: 0.39246529, g_loss: 1.75178337, rnn_loss: 0.00000000\n",
      " ** Epoch 148 took 81.567663s\n",
      "Epoch: [149/2000] time: 0.5906s, d_loss: 0.78264600, g_loss: 2.13891935, rnn_loss: 0.00000000\n",
      " ** Epoch 149 took 81.589064s\n",
      "Epoch: [150/2000] time: 0.6027s, d_loss: 0.85796893, g_loss: 1.01277995, rnn_loss: 0.00000000\n",
      " ** Epoch 150 took 81.579906s\n",
      "The checkpoint has been created.\n",
      "[*] Save checkpoints SUCCESS!\n",
      "Epoch: [151/2000] time: 0.5890s, d_loss: 0.50611627, g_loss: 0.11015547, rnn_loss: 0.00000000\n",
      " ** Epoch 151 took 81.651189s\n",
      "Epoch: [152/2000] time: 0.5938s, d_loss: 1.58111107, g_loss: 2.57692194, rnn_loss: 0.00000000\n",
      " ** Epoch 152 took 81.555819s\n",
      "Epoch: [153/2000] time: 0.5911s, d_loss: 0.65737009, g_loss: 0.81711453, rnn_loss: 0.00000000\n",
      " ** Epoch 153 took 81.708713s\n",
      "Epoch: [154/2000] time: 0.5927s, d_loss: 0.91571045, g_loss: 2.02162600, rnn_loss: 0.00000000\n",
      " ** Epoch 154 took 82.314040s\n",
      "Epoch: [155/2000] time: 0.5872s, d_loss: 1.20354891, g_loss: 1.73632169, rnn_loss: 0.00000000\n",
      " ** Epoch 155 took 82.279193s\n",
      "Epoch: [156/2000] time: 0.6038s, d_loss: 0.97471374, g_loss: 2.74812913, rnn_loss: 0.00000000\n",
      " ** Epoch 156 took 81.942464s\n",
      "Epoch: [157/2000] time: 0.6010s, d_loss: 0.61290377, g_loss: 3.00762439, rnn_loss: 0.00000000\n",
      " ** Epoch 157 took 82.623219s\n",
      "Epoch: [158/2000] time: 0.5915s, d_loss: 1.55185425, g_loss: -0.49688357, rnn_loss: 0.00000000\n",
      " ** Epoch 158 took 82.155425s\n",
      "Epoch: [159/2000] time: 0.5930s, d_loss: 0.29676470, g_loss: 2.30973673, rnn_loss: 0.00000000\n",
      " ** Epoch 159 took 81.908062s\n",
      "Epoch: [160/2000] time: 0.5892s, d_loss: 0.27713165, g_loss: 0.61279762, rnn_loss: 0.00000000\n",
      " ** Epoch 160 took 82.344264s\n",
      "Epoch: [161/2000] time: 0.5960s, d_loss: 1.58075452, g_loss: 0.02624168, rnn_loss: 0.00000000\n",
      " ** Epoch 161 took 81.977984s\n",
      "Epoch: [162/2000] time: 0.5906s, d_loss: 0.42976290, g_loss: 2.69940281, rnn_loss: 0.00000000\n",
      " ** Epoch 162 took 82.067249s\n",
      "Epoch: [163/2000] time: 0.6071s, d_loss: 0.59146154, g_loss: 2.22965050, rnn_loss: 0.00000000\n",
      " ** Epoch 163 took 81.682273s\n",
      "Epoch: [164/2000] time: 0.5895s, d_loss: 1.03979886, g_loss: 2.66820002, rnn_loss: 0.00000000\n",
      " ** Epoch 164 took 81.612688s\n",
      "Epoch: [165/2000] time: 0.5915s, d_loss: 0.73214221, g_loss: 2.47290015, rnn_loss: 0.00000000\n",
      " ** Epoch 165 took 81.554052s\n",
      "Epoch: [166/2000] time: 0.5914s, d_loss: 1.12498760, g_loss: -0.33889329, rnn_loss: 0.00000000\n",
      " ** Epoch 166 took 81.846983s\n",
      "Epoch: [167/2000] time: 0.5894s, d_loss: 0.72558308, g_loss: 0.46796557, rnn_loss: 0.00000000\n",
      " ** Epoch 167 took 81.648991s\n",
      "Epoch: [168/2000] time: 0.5898s, d_loss: 0.27641034, g_loss: 1.79661572, rnn_loss: 0.00000000\n",
      " ** Epoch 168 took 81.675597s\n",
      "Epoch: [169/2000] time: 0.5919s, d_loss: 0.52275944, g_loss: 2.48297453, rnn_loss: 0.00000000\n",
      " ** Epoch 169 took 82.746834s\n",
      "Epoch: [170/2000] time: 0.5934s, d_loss: 1.07384908, g_loss: -0.42887354, rnn_loss: 0.00000000\n",
      " ** Epoch 170 took 81.964088s\n",
      "Epoch: [171/2000] time: 0.5906s, d_loss: 0.91541076, g_loss: 2.51280928, rnn_loss: 0.00000000\n",
      " ** Epoch 171 took 82.559292s\n",
      "Epoch: [172/2000] time: 0.5885s, d_loss: 0.83893108, g_loss: 1.72168398, rnn_loss: 0.00000000\n",
      " ** Epoch 172 took 81.569863s\n",
      "Epoch: [173/2000] time: 0.5958s, d_loss: 0.77214080, g_loss: 2.64779043, rnn_loss: 0.00000000\n",
      " ** Epoch 173 took 82.738886s\n",
      "Epoch: [174/2000] time: 0.5926s, d_loss: 0.45706612, g_loss: 0.34313697, rnn_loss: 0.00000000\n",
      " ** Epoch 174 took 82.526536s\n",
      "Epoch: [175/2000] time: 0.5893s, d_loss: 1.13672924, g_loss: 0.04556948, rnn_loss: 0.00000000\n",
      " ** Epoch 175 took 82.052262s\n",
      "Epoch: [176/2000] time: 0.5905s, d_loss: 0.21807137, g_loss: 0.71465611, rnn_loss: 0.00000000\n",
      " ** Epoch 176 took 81.831782s\n",
      "Epoch: [177/2000] time: 0.5980s, d_loss: 0.57799685, g_loss: 0.33640981, rnn_loss: 0.00000000\n",
      " ** Epoch 177 took 82.167877s\n",
      "Epoch: [178/2000] time: 0.5968s, d_loss: 0.54012102, g_loss: 2.55037546, rnn_loss: 0.00000000\n",
      " ** Epoch 178 took 82.189458s\n",
      "Epoch: [179/2000] time: 0.5900s, d_loss: 0.95719600, g_loss: 1.93597698, rnn_loss: 0.00000000\n",
      " ** Epoch 179 took 81.895800s\n",
      "Epoch: [180/2000] time: 0.5914s, d_loss: 0.32862768, g_loss: 2.06643915, rnn_loss: 0.00000000\n",
      " ** Epoch 180 took 81.886353s\n",
      "Epoch: [181/2000] time: 0.5933s, d_loss: 1.21610308, g_loss: 1.37904191, rnn_loss: 0.00000000\n",
      " ** Epoch 181 took 82.243357s\n",
      "Epoch: [182/2000] time: 0.5959s, d_loss: 1.04658580, g_loss: 0.50227904, rnn_loss: 0.00000000\n",
      " ** Epoch 182 took 82.991566s\n",
      "Epoch: [183/2000] time: 0.5906s, d_loss: 0.26590863, g_loss: 2.53260326, rnn_loss: 0.00000000\n",
      " ** Epoch 183 took 82.108961s\n",
      "Epoch: [184/2000] time: 0.6052s, d_loss: 0.43318436, g_loss: 2.28561807, rnn_loss: 0.00000000\n",
      " ** Epoch 184 took 82.255790s\n",
      "Epoch: [185/2000] time: 0.5990s, d_loss: 1.02399457, g_loss: 1.96667922, rnn_loss: 0.00000000\n",
      " ** Epoch 185 took 82.303066s\n",
      "Epoch: [186/2000] time: 0.6015s, d_loss: 0.43606037, g_loss: 0.42048866, rnn_loss: 0.00000000\n",
      " ** Epoch 186 took 82.513962s\n",
      "Epoch: [187/2000] time: 0.5950s, d_loss: 0.60953778, g_loss: 2.16215229, rnn_loss: 0.00000000\n",
      " ** Epoch 187 took 82.304080s\n",
      "Epoch: [188/2000] time: 0.5957s, d_loss: 0.60496330, g_loss: 1.09144068, rnn_loss: 0.00000000\n",
      " ** Epoch 188 took 82.295524s\n",
      "Epoch: [189/2000] time: 0.5979s, d_loss: 1.10717225, g_loss: 1.55828369, rnn_loss: 0.00000000\n",
      " ** Epoch 189 took 82.135753s\n",
      "Epoch: [190/2000] time: 0.5936s, d_loss: 0.71761960, g_loss: 1.22620082, rnn_loss: 0.00000000\n",
      " ** Epoch 190 took 82.166948s\n",
      "Epoch: [191/2000] time: 0.5965s, d_loss: 0.52679896, g_loss: 1.70390308, rnn_loss: 0.00000000\n",
      " ** Epoch 191 took 82.044643s\n",
      "Epoch: [192/2000] time: 0.5980s, d_loss: 0.78624856, g_loss: 3.57748413, rnn_loss: 0.00000000\n",
      " ** Epoch 192 took 82.401484s\n",
      "Epoch: [193/2000] time: 0.5932s, d_loss: 2.21836591, g_loss: -0.10473954, rnn_loss: 0.00000000\n",
      " ** Epoch 193 took 82.228383s\n",
      "Epoch: [194/2000] time: 0.5979s, d_loss: 1.32925236, g_loss: -0.69135118, rnn_loss: 0.00000000\n",
      " ** Epoch 194 took 82.105140s\n",
      "Epoch: [195/2000] time: 0.5933s, d_loss: 0.57413816, g_loss: 1.73790145, rnn_loss: 0.00000000\n",
      " ** Epoch 195 took 82.117336s\n",
      "Epoch: [196/2000] time: 0.5980s, d_loss: 1.41141188, g_loss: 2.92678165, rnn_loss: 0.00000000\n",
      " ** Epoch 196 took 82.545938s\n",
      "Epoch: [197/2000] time: 0.5931s, d_loss: 0.56655252, g_loss: 0.36071643, rnn_loss: 0.00000000\n",
      " ** Epoch 197 took 82.305482s\n",
      "Epoch: [198/2000] time: 0.5968s, d_loss: 0.62258798, g_loss: 2.42195725, rnn_loss: 0.00000000\n",
      " ** Epoch 198 took 82.158744s\n",
      "Epoch: [199/2000] time: 0.6095s, d_loss: 0.74070358, g_loss: 2.62416220, rnn_loss: 0.00000000\n",
      " ** Epoch 199 took 82.101705s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [200/2000] time: 0.5952s, d_loss: 0.33186769, g_loss: 1.23734665, rnn_loss: 0.00000000\n",
      " ** Epoch 200 took 82.245413s\n",
      "The checkpoint has been created.\n",
      "[*] Save checkpoints SUCCESS!\n",
      "Epoch: [201/2000] time: 0.5898s, d_loss: 0.18040176, g_loss: 2.53436160, rnn_loss: 0.00000000\n",
      " ** Epoch 201 took 81.941207s\n",
      "Epoch: [202/2000] time: 0.5949s, d_loss: 0.61766064, g_loss: 2.68062115, rnn_loss: 0.00000000\n",
      " ** Epoch 202 took 82.187438s\n",
      "Epoch: [203/2000] time: 0.5929s, d_loss: 1.10656798, g_loss: 1.81166744, rnn_loss: 0.00000000\n",
      " ** Epoch 203 took 82.194505s\n",
      "Epoch: [204/2000] time: 0.5962s, d_loss: 0.73760515, g_loss: 3.64925241, rnn_loss: 0.00000000\n",
      " ** Epoch 204 took 82.697806s\n",
      "Epoch: [205/2000] time: 0.5958s, d_loss: 0.75053614, g_loss: 1.84230316, rnn_loss: 0.00000000\n",
      " ** Epoch 205 took 82.191838s\n",
      "Epoch: [206/2000] time: 0.5946s, d_loss: 0.15967757, g_loss: 2.28901958, rnn_loss: 0.00000000\n",
      " ** Epoch 206 took 81.978326s\n",
      "Epoch: [207/2000] time: 0.6000s, d_loss: 0.91317111, g_loss: 2.61390495, rnn_loss: 0.00000000\n",
      " ** Epoch 207 took 82.430833s\n",
      "Epoch: [208/2000] time: 0.5941s, d_loss: 0.70048833, g_loss: 3.72223282, rnn_loss: 0.00000000\n",
      " ** Epoch 208 took 82.254897s\n",
      "Epoch: [209/2000] time: 0.5985s, d_loss: 0.83980954, g_loss: -0.52473366, rnn_loss: 0.00000000\n",
      " ** Epoch 209 took 82.141597s\n",
      "Epoch: [210/2000] time: 0.5883s, d_loss: 0.65241349, g_loss: 3.19357300, rnn_loss: 0.00000000\n",
      " ** Epoch 210 took 82.079141s\n",
      "Epoch: [211/2000] time: 0.5944s, d_loss: 0.34192622, g_loss: 0.81217742, rnn_loss: 0.00000000\n",
      " ** Epoch 211 took 82.548155s\n",
      "Epoch: [212/2000] time: 0.5970s, d_loss: 0.89717788, g_loss: 2.29988766, rnn_loss: 0.00000000\n",
      " ** Epoch 212 took 82.370402s\n",
      "Epoch: [213/2000] time: 0.5956s, d_loss: 0.67668116, g_loss: 1.83026052, rnn_loss: 0.00000000\n",
      " ** Epoch 213 took 82.669654s\n",
      "Epoch: [214/2000] time: 0.5943s, d_loss: 0.67247283, g_loss: 2.29453301, rnn_loss: 0.00000000\n",
      " ** Epoch 214 took 82.117240s\n",
      "Epoch: [215/2000] time: 0.5956s, d_loss: 0.65733814, g_loss: 2.66164255, rnn_loss: 0.00000000\n",
      " ** Epoch 215 took 82.371407s\n",
      "Epoch: [216/2000] time: 0.5930s, d_loss: 0.74547076, g_loss: 2.26303601, rnn_loss: 0.00000000\n",
      " ** Epoch 216 took 81.933768s\n",
      "Epoch: [217/2000] time: 0.5939s, d_loss: 0.16006684, g_loss: 2.04790735, rnn_loss: 0.00000000\n",
      " ** Epoch 217 took 81.935289s\n",
      "Epoch: [218/2000] time: 0.5941s, d_loss: 0.28121191, g_loss: 2.22915983, rnn_loss: 0.00000000\n",
      " ** Epoch 218 took 82.153070s\n",
      "Epoch: [219/2000] time: 0.5939s, d_loss: 0.91095257, g_loss: -0.31637034, rnn_loss: 0.00000000\n",
      " ** Epoch 219 took 82.298833s\n",
      "Epoch: [220/2000] time: 0.5931s, d_loss: 0.65488911, g_loss: 1.30586708, rnn_loss: 0.00000000\n",
      " ** Epoch 220 took 81.748522s\n",
      "Epoch: [221/2000] time: 0.5887s, d_loss: 0.28447860, g_loss: 3.19937086, rnn_loss: 0.00000000\n",
      " ** Epoch 221 took 81.807571s\n",
      "Epoch: [222/2000] time: 0.5909s, d_loss: 0.68554163, g_loss: 2.85685682, rnn_loss: 0.00000000\n",
      " ** Epoch 222 took 82.228536s\n",
      "Epoch: [223/2000] time: 0.5921s, d_loss: 0.41544488, g_loss: 3.75288677, rnn_loss: 0.00000000\n",
      " ** Epoch 223 took 82.264268s\n",
      "Epoch: [224/2000] time: 0.5939s, d_loss: 0.68594801, g_loss: 0.79499638, rnn_loss: 0.00000000\n",
      " ** Epoch 224 took 82.130107s\n",
      "Epoch: [225/2000] time: 0.5926s, d_loss: 0.04637741, g_loss: 1.99252629, rnn_loss: 0.00000000\n",
      " ** Epoch 225 took 82.053924s\n",
      "Epoch: [226/2000] time: 0.6021s, d_loss: 0.87509114, g_loss: 2.15105343, rnn_loss: 0.00000000\n",
      " ** Epoch 226 took 82.306505s\n",
      "Epoch: [227/2000] time: 0.5947s, d_loss: 1.32808077, g_loss: 2.61087871, rnn_loss: 0.00000000\n",
      " ** Epoch 227 took 81.817788s\n",
      "Epoch: [228/2000] time: 0.6023s, d_loss: 0.50639933, g_loss: -0.16617206, rnn_loss: 0.00000000\n",
      " ** Epoch 228 took 81.868402s\n",
      "Epoch: [229/2000] time: 0.5922s, d_loss: 0.64589393, g_loss: 0.37451604, rnn_loss: 0.00000000\n",
      " ** Epoch 229 took 81.876184s\n",
      "Epoch: [230/2000] time: 0.5933s, d_loss: 0.35645998, g_loss: 0.69403005, rnn_loss: 0.00000000\n",
      " ** Epoch 230 took 82.224846s\n",
      "Epoch: [231/2000] time: 0.5936s, d_loss: 0.97270763, g_loss: -0.25546733, rnn_loss: 0.00000000\n",
      " ** Epoch 231 took 81.693016s\n",
      "Epoch: [232/2000] time: 0.5976s, d_loss: 0.12514172, g_loss: 3.12410212, rnn_loss: 0.00000000\n",
      " ** Epoch 232 took 81.658891s\n",
      "Epoch: [233/2000] time: 0.6051s, d_loss: 0.97024876, g_loss: 2.28616858, rnn_loss: 0.00000000\n",
      " ** Epoch 233 took 82.082284s\n",
      "Epoch: [234/2000] time: 0.6015s, d_loss: 0.54103506, g_loss: 0.99812716, rnn_loss: 0.00000000\n",
      " ** Epoch 234 took 82.669833s\n",
      "Epoch: [235/2000] time: 0.5914s, d_loss: 0.52045137, g_loss: 1.83814692, rnn_loss: 0.00000000\n",
      " ** Epoch 235 took 81.944105s\n",
      "Epoch: [236/2000] time: 0.5892s, d_loss: 0.50156093, g_loss: 1.38447142, rnn_loss: 0.00000000\n",
      " ** Epoch 236 took 81.726247s\n",
      "Epoch: [237/2000] time: 0.5973s, d_loss: 0.76441199, g_loss: 3.08657289, rnn_loss: 0.00000000\n",
      " ** Epoch 237 took 82.127348s\n",
      "Epoch: [238/2000] time: 0.5943s, d_loss: 0.42738187, g_loss: 1.94188321, rnn_loss: 0.00000000\n",
      " ** Epoch 238 took 82.435964s\n",
      "Epoch: [239/2000] time: 0.6166s, d_loss: 0.28464597, g_loss: 2.55929184, rnn_loss: 0.00000000\n",
      " ** Epoch 239 took 82.858049s\n",
      "Epoch: [240/2000] time: 0.6077s, d_loss: 0.15353140, g_loss: 1.07418835, rnn_loss: 0.00000000\n",
      " ** Epoch 240 took 83.232681s\n",
      "Epoch: [241/2000] time: 0.5902s, d_loss: 0.21248806, g_loss: 3.02062178, rnn_loss: 0.00000000\n",
      " ** Epoch 241 took 82.638460s\n",
      "Epoch: [242/2000] time: 0.5928s, d_loss: 0.26002246, g_loss: 2.71742201, rnn_loss: 0.00000000\n",
      " ** Epoch 242 took 82.170565s\n",
      "Epoch: [243/2000] time: 0.5884s, d_loss: 0.15200621, g_loss: 2.99186945, rnn_loss: 0.00000000\n",
      " ** Epoch 243 took 82.032706s\n",
      "Epoch: [244/2000] time: 0.5992s, d_loss: 0.76672697, g_loss: 2.87331963, rnn_loss: 0.00000000\n",
      " ** Epoch 244 took 82.058134s\n",
      "Epoch: [245/2000] time: 0.5920s, d_loss: 0.34312290, g_loss: 1.04546785, rnn_loss: 0.00000000\n",
      " ** Epoch 245 took 82.325376s\n",
      "Epoch: [246/2000] time: 0.5961s, d_loss: 0.54442328, g_loss: 0.57965672, rnn_loss: 0.00000000\n",
      " ** Epoch 246 took 82.066143s\n",
      "Epoch: [247/2000] time: 0.5936s, d_loss: 0.31785002, g_loss: 0.45853081, rnn_loss: 0.00000000\n",
      " ** Epoch 247 took 82.234477s\n",
      "Epoch: [248/2000] time: 0.6079s, d_loss: 0.27892894, g_loss: 0.92188740, rnn_loss: 0.00000000\n",
      " ** Epoch 248 took 82.067938s\n",
      "Epoch: [249/2000] time: 0.6032s, d_loss: 0.42089552, g_loss: 3.43684626, rnn_loss: 0.00000000\n",
      " ** Epoch 249 took 82.449228s\n",
      "Epoch: [250/2000] time: 0.5905s, d_loss: 0.24431764, g_loss: 0.48977911, rnn_loss: 0.00000000\n",
      " ** Epoch 250 took 81.998370s\n",
      "The checkpoint has been created.\n",
      "[*] Save checkpoints SUCCESS!\n",
      "Epoch: [251/2000] time: 0.6077s, d_loss: 0.30981100, g_loss: 2.05994296, rnn_loss: 0.00000000\n",
      " ** Epoch 251 took 82.133940s\n",
      "Epoch: [252/2000] time: 0.5991s, d_loss: 0.03168148, g_loss: 1.92657685, rnn_loss: 0.00000000\n",
      " ** Epoch 252 took 82.318643s\n",
      "Epoch: [253/2000] time: 0.5910s, d_loss: 1.05124235, g_loss: 2.29174614, rnn_loss: 0.00000000\n",
      " ** Epoch 253 took 82.019066s\n",
      "Epoch: [254/2000] time: 0.5945s, d_loss: 0.13662547, g_loss: 1.27347541, rnn_loss: 0.00000000\n",
      " ** Epoch 254 took 83.006477s\n",
      "Epoch: [255/2000] time: 0.6005s, d_loss: 0.95208007, g_loss: -0.11013284, rnn_loss: 0.00000000\n",
      " ** Epoch 255 took 82.692571s\n",
      "Epoch: [256/2000] time: 0.5952s, d_loss: 0.05415688, g_loss: 1.69347191, rnn_loss: 0.00000000\n",
      " ** Epoch 256 took 83.002601s\n",
      "Epoch: [257/2000] time: 0.6013s, d_loss: 0.98106289, g_loss: 1.42884731, rnn_loss: 0.00000000\n",
      " ** Epoch 257 took 82.489608s\n",
      "Epoch: [258/2000] time: 0.5950s, d_loss: 0.65091258, g_loss: 0.98988390, rnn_loss: 0.00000000\n",
      " ** Epoch 258 took 82.540199s\n",
      "Epoch: [259/2000] time: 0.6052s, d_loss: 0.79429495, g_loss: 2.04405904, rnn_loss: 0.00000000\n",
      " ** Epoch 259 took 82.316414s\n",
      "Epoch: [260/2000] time: 0.6012s, d_loss: 0.47287256, g_loss: 3.68972778, rnn_loss: 0.00000000\n",
      " ** Epoch 260 took 82.788293s\n",
      "Epoch: [261/2000] time: 0.5907s, d_loss: 1.87783265, g_loss: 3.58129549, rnn_loss: 0.00000000\n",
      " ** Epoch 261 took 82.469672s\n",
      "Epoch: [262/2000] time: 0.6004s, d_loss: 0.64744353, g_loss: 0.84356797, rnn_loss: 0.00000000\n",
      " ** Epoch 262 took 82.375100s\n",
      "Epoch: [263/2000] time: 0.6417s, d_loss: 0.30336100, g_loss: 3.59852695, rnn_loss: 0.00000000\n",
      " ** Epoch 263 took 82.619827s\n",
      "Epoch: [264/2000] time: 0.6079s, d_loss: 0.90694213, g_loss: -0.14159243, rnn_loss: 0.00000000\n",
      " ** Epoch 264 took 82.744254s\n",
      "Epoch: [265/2000] time: 0.5978s, d_loss: 0.57460982, g_loss: 3.52119970, rnn_loss: 0.00000000\n",
      " ** Epoch 265 took 82.749245s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [266/2000] time: 0.6076s, d_loss: 0.62846118, g_loss: 0.26538214, rnn_loss: 0.00000000\n",
      " ** Epoch 266 took 82.667799s\n",
      "Epoch: [267/2000] time: 0.6014s, d_loss: 0.77930200, g_loss: 3.47288418, rnn_loss: 0.00000000\n",
      " ** Epoch 267 took 82.965747s\n",
      "Epoch: [268/2000] time: 0.5922s, d_loss: 0.33471602, g_loss: 0.93282878, rnn_loss: 0.00000000\n",
      " ** Epoch 268 took 82.314738s\n",
      "Epoch: [269/2000] time: 0.5955s, d_loss: 0.74607128, g_loss: 4.35686159, rnn_loss: 0.00000000\n",
      " ** Epoch 269 took 82.259747s\n",
      "Epoch: [270/2000] time: 0.5963s, d_loss: 0.84189522, g_loss: 0.73834956, rnn_loss: 0.00000000\n",
      " ** Epoch 270 took 82.334436s\n",
      "Epoch: [271/2000] time: 0.5884s, d_loss: 0.66490871, g_loss: 3.78460550, rnn_loss: 0.00000000\n",
      " ** Epoch 271 took 82.772235s\n",
      "Epoch: [272/2000] time: 0.5974s, d_loss: 0.43351683, g_loss: 2.59260941, rnn_loss: 0.00000000\n",
      " ** Epoch 272 took 82.316700s\n",
      "Epoch: [273/2000] time: 0.6035s, d_loss: 0.22562337, g_loss: 2.77558899, rnn_loss: 0.00000000\n",
      " ** Epoch 273 took 82.183624s\n",
      "Epoch: [274/2000] time: 0.5979s, d_loss: 0.91784334, g_loss: 1.51642227, rnn_loss: 0.00000000\n",
      " ** Epoch 274 took 82.311378s\n",
      "Epoch: [275/2000] time: 0.5926s, d_loss: 0.42981911, g_loss: 2.03845596, rnn_loss: 0.00000000\n",
      " ** Epoch 275 took 82.841628s\n",
      "Epoch: [276/2000] time: 0.5926s, d_loss: 0.11265825, g_loss: 2.25861454, rnn_loss: 0.00000000\n",
      " ** Epoch 276 took 82.137680s\n",
      "Epoch: [277/2000] time: 0.5955s, d_loss: 0.11915152, g_loss: 1.99537802, rnn_loss: 0.00000000\n",
      " ** Epoch 277 took 82.075170s\n",
      "Epoch: [278/2000] time: 0.5930s, d_loss: 0.15162192, g_loss: 3.48460126, rnn_loss: 0.00000000\n",
      " ** Epoch 278 took 82.242937s\n",
      "Epoch: [279/2000] time: 0.5995s, d_loss: 0.11127798, g_loss: 1.45669627, rnn_loss: 0.00000000\n",
      " ** Epoch 279 took 82.676839s\n",
      "Epoch: [280/2000] time: 0.5952s, d_loss: 0.42348212, g_loss: 1.41317821, rnn_loss: 0.00000000\n",
      " ** Epoch 280 took 82.266190s\n",
      "Epoch: [281/2000] time: 0.5961s, d_loss: 0.50676173, g_loss: 1.93452024, rnn_loss: 0.00000000\n",
      " ** Epoch 281 took 82.328747s\n",
      "Epoch: [282/2000] time: 0.5911s, d_loss: 0.68892151, g_loss: 0.72758603, rnn_loss: 0.00000000\n",
      " ** Epoch 282 took 82.758281s\n",
      "Epoch: [283/2000] time: 0.5980s, d_loss: 0.38031992, g_loss: 3.66259480, rnn_loss: 0.00000000\n",
      " ** Epoch 283 took 82.276275s\n",
      "Epoch: [284/2000] time: 0.5889s, d_loss: 1.56070387, g_loss: 1.85395861, rnn_loss: 0.00000000\n",
      " ** Epoch 284 took 82.149702s\n",
      "Epoch: [285/2000] time: 0.5946s, d_loss: 0.60035396, g_loss: 4.61247063, rnn_loss: 0.00000000\n",
      " ** Epoch 285 took 82.155872s\n",
      "Epoch: [286/2000] time: 0.5922s, d_loss: 0.46712857, g_loss: 1.40156174, rnn_loss: 0.00000000\n",
      " ** Epoch 286 took 82.729167s\n",
      "Epoch: [287/2000] time: 0.5974s, d_loss: 0.63314545, g_loss: -0.13411620, rnn_loss: 0.00000000\n",
      " ** Epoch 287 took 82.557534s\n",
      "Epoch: [288/2000] time: 0.5932s, d_loss: 0.31120881, g_loss: -0.06519312, rnn_loss: 0.00000000\n",
      " ** Epoch 288 took 82.618064s\n",
      "Epoch: [289/2000] time: 0.6012s, d_loss: 0.70649612, g_loss: -0.25261194, rnn_loss: 0.00000000\n",
      " ** Epoch 289 took 82.733020s\n",
      "Epoch: [290/2000] time: 0.6030s, d_loss: 0.48914698, g_loss: 3.39297485, rnn_loss: 0.00000000\n",
      " ** Epoch 290 took 82.828591s\n",
      "Epoch: [291/2000] time: 0.5939s, d_loss: 0.36524233, g_loss: 3.75329876, rnn_loss: 0.00000000\n",
      " ** Epoch 291 took 82.447822s\n",
      "Epoch: [292/2000] time: 0.5985s, d_loss: 0.08733530, g_loss: 2.52793527, rnn_loss: 0.00000000\n",
      " ** Epoch 292 took 82.357124s\n",
      "Epoch: [293/2000] time: 0.5958s, d_loss: 0.71349329, g_loss: 4.05012941, rnn_loss: 0.00000000\n",
      " ** Epoch 293 took 82.423939s\n",
      "Epoch: [294/2000] time: 0.5951s, d_loss: 0.97238076, g_loss: 3.58025885, rnn_loss: 0.00000000\n",
      " ** Epoch 294 took 82.567007s\n",
      "Epoch: [295/2000] time: 0.5902s, d_loss: 0.44129670, g_loss: 0.15607010, rnn_loss: 0.00000000\n",
      " ** Epoch 295 took 82.382523s\n",
      "Epoch: [296/2000] time: 0.5961s, d_loss: 1.51431823, g_loss: 0.73296535, rnn_loss: 0.00000000\n",
      " ** Epoch 296 took 82.288576s\n",
      "Epoch: [297/2000] time: 0.5939s, d_loss: 0.38549572, g_loss: 4.04714584, rnn_loss: 0.00000000\n",
      " ** Epoch 297 took 82.532402s\n",
      "Epoch: [298/2000] time: 0.5970s, d_loss: 0.85004175, g_loss: 2.98144293, rnn_loss: 0.00000000\n",
      " ** Epoch 298 took 82.251293s\n",
      "Epoch: [299/2000] time: 0.5913s, d_loss: 0.11379434, g_loss: 3.21250749, rnn_loss: 0.00000000\n",
      " ** Epoch 299 took 82.137323s\n",
      "Epoch: [300/2000] time: 0.5974s, d_loss: 0.67239505, g_loss: 0.12727633, rnn_loss: 0.00000000\n",
      " ** Epoch 300 took 82.177236s\n",
      "The checkpoint has been created.\n",
      "[*] Save checkpoints SUCCESS!\n",
      "Epoch: [301/2000] time: 0.6028s, d_loss: 1.25137687, g_loss: 2.65163732, rnn_loss: 0.00000000\n",
      " ** Epoch 301 took 82.544264s\n",
      "Epoch: [302/2000] time: 0.5982s, d_loss: 0.48230511, g_loss: 1.20601678, rnn_loss: 0.00000000\n",
      " ** Epoch 302 took 82.250458s\n",
      "Epoch: [303/2000] time: 0.5984s, d_loss: 0.16561851, g_loss: 1.60223055, rnn_loss: 0.00000000\n",
      " ** Epoch 303 took 82.476201s\n",
      "Epoch: [304/2000] time: 0.5932s, d_loss: 1.38613939, g_loss: 3.20705819, rnn_loss: 0.00000000\n",
      " ** Epoch 304 took 82.217478s\n",
      "Epoch: [305/2000] time: 0.5985s, d_loss: 0.62299359, g_loss: 3.45112038, rnn_loss: 0.00000000\n",
      " ** Epoch 305 took 82.391178s\n",
      "Epoch: [306/2000] time: 0.5930s, d_loss: 0.30355251, g_loss: 0.48478970, rnn_loss: 0.00000000\n",
      " ** Epoch 306 took 81.989100s\n",
      "Epoch: [307/2000] time: 0.5972s, d_loss: 0.18763816, g_loss: 1.22028828, rnn_loss: 0.00000000\n",
      " ** Epoch 307 took 82.163923s\n",
      "Epoch: [308/2000] time: 0.5986s, d_loss: 1.05025530, g_loss: 3.14902163, rnn_loss: 0.00000000\n",
      " ** Epoch 308 took 82.458312s\n",
      "Epoch: [309/2000] time: 0.5922s, d_loss: 0.35545096, g_loss: 1.18805373, rnn_loss: 0.00000000\n",
      " ** Epoch 309 took 82.676882s\n",
      "Epoch: [310/2000] time: 0.5923s, d_loss: 0.49391949, g_loss: 2.30319285, rnn_loss: 0.00000000\n",
      " ** Epoch 310 took 82.335785s\n",
      "Epoch: [311/2000] time: 0.6097s, d_loss: 1.54422128, g_loss: 3.70543599, rnn_loss: 0.00000000\n",
      " ** Epoch 311 took 82.527827s\n",
      "Epoch: [312/2000] time: 0.5930s, d_loss: 0.43932375, g_loss: 3.30003309, rnn_loss: 0.00000000\n",
      " ** Epoch 312 took 82.324814s\n",
      "Epoch: [313/2000] time: 0.5913s, d_loss: 0.12221307, g_loss: 3.00645018, rnn_loss: 0.00000000\n",
      " ** Epoch 313 took 82.115677s\n",
      "Epoch: [314/2000] time: 0.5956s, d_loss: 0.76629680, g_loss: -0.21118368, rnn_loss: 0.00000000\n",
      " ** Epoch 314 took 82.018577s\n",
      "Epoch: [315/2000] time: 0.5942s, d_loss: 0.32546809, g_loss: 3.70452428, rnn_loss: 0.00000000\n",
      " ** Epoch 315 took 82.205641s\n",
      "Epoch: [316/2000] time: 0.5959s, d_loss: 0.02875509, g_loss: 1.89185882, rnn_loss: 0.00000000\n",
      " ** Epoch 316 took 82.217707s\n",
      "Epoch: [317/2000] time: 0.5900s, d_loss: 0.33689684, g_loss: 2.78677058, rnn_loss: 0.00000000\n",
      " ** Epoch 317 took 81.967021s\n",
      "Epoch: [318/2000] time: 0.5941s, d_loss: 1.35199916, g_loss: 2.90274334, rnn_loss: 0.00000000\n",
      " ** Epoch 318 took 82.033829s\n",
      "Epoch: [319/2000] time: 0.5907s, d_loss: 0.78025007, g_loss: 3.07169199, rnn_loss: 0.00000000\n",
      " ** Epoch 319 took 82.275369s\n",
      "Epoch: [320/2000] time: 0.6009s, d_loss: 0.08248061, g_loss: 1.63253903, rnn_loss: 0.00000000\n",
      " ** Epoch 320 took 82.437516s\n",
      "Epoch: [321/2000] time: 0.6029s, d_loss: 0.18395878, g_loss: 1.16127515, rnn_loss: 0.00000000\n",
      " ** Epoch 321 took 82.170730s\n",
      "Epoch: [322/2000] time: 0.5996s, d_loss: 0.12001102, g_loss: 1.43953860, rnn_loss: 0.00000000\n",
      " ** Epoch 322 took 83.295910s\n",
      "Epoch: [323/2000] time: 0.5921s, d_loss: 0.92247808, g_loss: 3.28366375, rnn_loss: 0.00000000\n",
      " ** Epoch 323 took 82.910961s\n",
      "Epoch: [324/2000] time: 0.5976s, d_loss: 0.11721675, g_loss: 1.20807004, rnn_loss: 0.00000000\n",
      " ** Epoch 324 took 82.907217s\n",
      "Epoch: [325/2000] time: 0.5954s, d_loss: 0.06659470, g_loss: 2.09227991, rnn_loss: 0.00000000\n",
      " ** Epoch 325 took 82.687149s\n",
      "Epoch: [326/2000] time: 0.5947s, d_loss: 0.46289274, g_loss: 3.30650616, rnn_loss: 0.00000000\n",
      " ** Epoch 326 took 82.605741s\n",
      "Epoch: [327/2000] time: 0.5943s, d_loss: 0.35300475, g_loss: 2.38770509, rnn_loss: 0.00000000\n",
      " ** Epoch 327 took 82.792297s\n",
      "Epoch: [328/2000] time: 0.5953s, d_loss: 0.26681805, g_loss: 1.25661683, rnn_loss: 0.00000000\n",
      " ** Epoch 328 took 82.489224s\n",
      "Epoch: [329/2000] time: 0.5947s, d_loss: 0.75200200, g_loss: 3.22345805, rnn_loss: 0.00000000\n",
      " ** Epoch 329 took 82.386233s\n",
      "Epoch: [330/2000] time: 0.5930s, d_loss: 0.07240125, g_loss: 1.74708164, rnn_loss: 0.00000000\n",
      " ** Epoch 330 took 82.499074s\n",
      "Epoch: [331/2000] time: 0.5976s, d_loss: 0.85239196, g_loss: 4.43321609, rnn_loss: 0.00000000\n",
      " ** Epoch 331 took 82.918108s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [332/2000] time: 0.5957s, d_loss: 0.78358150, g_loss: 0.42208704, rnn_loss: 0.00000000\n",
      " ** Epoch 332 took 82.501358s\n",
      "Epoch: [333/2000] time: 0.5928s, d_loss: 0.11717428, g_loss: 3.26254511, rnn_loss: 0.00000000\n",
      " ** Epoch 333 took 82.367194s\n",
      "Epoch: [334/2000] time: 0.5995s, d_loss: 0.26039308, g_loss: 2.74592733, rnn_loss: 0.00000000\n",
      " ** Epoch 334 took 82.347531s\n",
      "Epoch: [335/2000] time: 0.6017s, d_loss: 1.07354796, g_loss: 2.72179556, rnn_loss: 0.00000000\n",
      " ** Epoch 335 took 82.646213s\n",
      "Epoch: [336/2000] time: 0.5925s, d_loss: 0.11353476, g_loss: 3.19148397, rnn_loss: 0.00000000\n",
      " ** Epoch 336 took 82.351674s\n",
      "Epoch: [337/2000] time: 0.5984s, d_loss: 0.43115282, g_loss: 4.46650410, rnn_loss: 0.00000000\n",
      " ** Epoch 337 took 82.295585s\n",
      "Epoch: [338/2000] time: 0.5996s, d_loss: 0.88394755, g_loss: 3.29506373, rnn_loss: 0.00000000\n",
      " ** Epoch 338 took 81.867448s\n",
      "Epoch: [339/2000] time: 0.5979s, d_loss: 1.08222270, g_loss: 3.36529684, rnn_loss: 0.00000000\n",
      " ** Epoch 339 took 82.682969s\n",
      "Epoch: [340/2000] time: 0.5979s, d_loss: 0.93355888, g_loss: 3.96552300, rnn_loss: 0.00000000\n",
      " ** Epoch 340 took 82.095921s\n",
      "Epoch: [341/2000] time: 0.5943s, d_loss: 1.07705069, g_loss: 4.08254147, rnn_loss: 0.00000000\n",
      " ** Epoch 341 took 82.164728s\n",
      "Epoch: [342/2000] time: 0.6091s, d_loss: 0.22234620, g_loss: 1.16991580, rnn_loss: 0.00000000\n",
      " ** Epoch 342 took 82.131524s\n",
      "Epoch: [343/2000] time: 0.5952s, d_loss: 0.39411134, g_loss: 1.28703213, rnn_loss: 0.00000000\n",
      " ** Epoch 343 took 82.077443s\n",
      "Epoch: [344/2000] time: 0.5904s, d_loss: 0.27724215, g_loss: 3.32379007, rnn_loss: 0.00000000\n",
      " ** Epoch 344 took 81.961534s\n",
      "Epoch: [345/2000] time: 0.5933s, d_loss: 0.27963150, g_loss: 4.49360609, rnn_loss: 0.00000000\n",
      " ** Epoch 345 took 82.063025s\n",
      "Epoch: [346/2000] time: 0.5976s, d_loss: 0.12058865, g_loss: 2.66414475, rnn_loss: 0.00000000\n",
      " ** Epoch 346 took 82.173820s\n",
      "Epoch: [347/2000] time: 0.5939s, d_loss: 0.53629792, g_loss: 4.13687181, rnn_loss: 0.00000000\n",
      " ** Epoch 347 took 81.966144s\n",
      "Epoch: [348/2000] time: 0.6032s, d_loss: 0.58734149, g_loss: 1.69186282, rnn_loss: 0.00000000\n",
      " ** Epoch 348 took 81.964514s\n",
      "Epoch: [349/2000] time: 0.5867s, d_loss: 0.62997407, g_loss: 4.23513699, rnn_loss: 0.00000000\n",
      " ** Epoch 349 took 81.916535s\n",
      "Epoch: [350/2000] time: 0.5891s, d_loss: 0.00466198, g_loss: 2.19243050, rnn_loss: 0.00000000\n",
      " ** Epoch 350 took 82.084296s\n",
      "The checkpoint has been created.\n",
      "[*] Save checkpoints SUCCESS!\n",
      "Epoch: [351/2000] time: 0.5937s, d_loss: 1.54549134, g_loss: 0.75423551, rnn_loss: 0.00000000\n",
      " ** Epoch 351 took 81.946966s\n",
      "Epoch: [352/2000] time: 0.5941s, d_loss: 0.07908731, g_loss: 2.50263023, rnn_loss: 0.00000000\n",
      " ** Epoch 352 took 81.964189s\n",
      "Epoch: [353/2000] time: 0.5958s, d_loss: 0.20604599, g_loss: 2.86084056, rnn_loss: 0.00000000\n",
      " ** Epoch 353 took 81.932765s\n",
      "Epoch: [354/2000] time: 0.5951s, d_loss: 0.50119275, g_loss: 3.98878956, rnn_loss: 0.00000000\n",
      " ** Epoch 354 took 81.997047s\n",
      "Epoch: [355/2000] time: 0.5898s, d_loss: 0.14584288, g_loss: 3.42700052, rnn_loss: 0.00000000\n",
      " ** Epoch 355 took 81.948668s\n",
      "Epoch: [356/2000] time: 0.5896s, d_loss: 0.03028095, g_loss: 2.32530499, rnn_loss: 0.00000000\n",
      " ** Epoch 356 took 81.953613s\n",
      "Epoch: [357/2000] time: 0.5989s, d_loss: 0.84964383, g_loss: 4.29467583, rnn_loss: 0.00000000\n",
      " ** Epoch 357 took 81.908417s\n",
      "Epoch: [358/2000] time: 0.5951s, d_loss: 0.53255904, g_loss: -0.42743427, rnn_loss: 0.00000000\n",
      " ** Epoch 358 took 82.113961s\n",
      "Epoch: [359/2000] time: 0.5937s, d_loss: 0.42850462, g_loss: 2.86224699, rnn_loss: 0.00000000\n",
      " ** Epoch 359 took 81.881183s\n",
      "Epoch: [360/2000] time: 0.5941s, d_loss: 0.06620096, g_loss: 1.83074808, rnn_loss: 0.00000000\n",
      " ** Epoch 360 took 81.928943s\n",
      "Epoch: [361/2000] time: 0.5930s, d_loss: 0.30130199, g_loss: 1.15784991, rnn_loss: 0.00000000\n",
      " ** Epoch 361 took 82.026763s\n",
      "Epoch: [362/2000] time: 0.5939s, d_loss: 0.64496934, g_loss: 3.56806445, rnn_loss: 0.00000000\n",
      " ** Epoch 362 took 81.880788s\n",
      "Epoch: [363/2000] time: 0.5949s, d_loss: 0.08234711, g_loss: 2.14919233, rnn_loss: 0.00000000\n",
      " ** Epoch 363 took 82.045907s\n",
      "Epoch: [364/2000] time: 0.5938s, d_loss: 0.26143628, g_loss: 3.35928345, rnn_loss: 0.00000000\n",
      " ** Epoch 364 took 81.913932s\n",
      "Epoch: [365/2000] time: 0.5949s, d_loss: 0.21866439, g_loss: 3.90704131, rnn_loss: 0.00000000\n",
      " ** Epoch 365 took 82.128178s\n",
      "Epoch: [366/2000] time: 0.5949s, d_loss: 1.29797852, g_loss: -0.32587197, rnn_loss: 0.00000000\n",
      " ** Epoch 366 took 81.860953s\n",
      "Epoch: [367/2000] time: 0.5905s, d_loss: 0.02394431, g_loss: 2.59942865, rnn_loss: 0.00000000\n",
      " ** Epoch 367 took 81.970819s\n",
      "Epoch: [368/2000] time: 0.5900s, d_loss: 0.37265849, g_loss: 0.14913584, rnn_loss: 0.00000000\n",
      " ** Epoch 368 took 81.739665s\n",
      "Epoch: [369/2000] time: 0.5874s, d_loss: 0.21916845, g_loss: 4.50837517, rnn_loss: 0.00000000\n",
      " ** Epoch 369 took 82.034262s\n",
      "Epoch: [370/2000] time: 0.5930s, d_loss: 0.27825192, g_loss: 4.12593842, rnn_loss: 0.00000000\n",
      " ** Epoch 370 took 81.683922s\n",
      "Epoch: [371/2000] time: 0.5941s, d_loss: 0.42092025, g_loss: 1.19979465, rnn_loss: 0.00000000\n",
      " ** Epoch 371 took 81.770791s\n",
      "Epoch: [372/2000] time: 0.5893s, d_loss: 0.87867939, g_loss: 0.10141049, rnn_loss: 0.00000000\n",
      " ** Epoch 372 took 81.762450s\n",
      "Epoch: [373/2000] time: 0.5924s, d_loss: 0.01096526, g_loss: 2.51778698, rnn_loss: 0.00000000\n",
      " ** Epoch 373 took 81.962585s\n",
      "Epoch: [374/2000] time: 0.6025s, d_loss: 0.00139340, g_loss: 2.60555983, rnn_loss: 0.00000000\n",
      " ** Epoch 374 took 81.759119s\n",
      "Epoch: [375/2000] time: 0.5941s, d_loss: 0.10686490, g_loss: 1.32908976, rnn_loss: 0.00000000\n",
      " ** Epoch 375 took 81.922513s\n",
      "Epoch: [376/2000] time: 0.6050s, d_loss: 0.87743169, g_loss: -0.33845028, rnn_loss: 0.00000000\n",
      " ** Epoch 376 took 81.992749s\n",
      "Epoch: [377/2000] time: 0.5920s, d_loss: 0.85016471, g_loss: 5.21287346, rnn_loss: 0.00000000\n",
      " ** Epoch 377 took 82.032676s\n",
      "Epoch: [378/2000] time: 0.5937s, d_loss: 0.26599839, g_loss: 3.50259876, rnn_loss: 0.00000000\n",
      " ** Epoch 378 took 81.986521s\n",
      "Epoch: [379/2000] time: 0.5918s, d_loss: 0.22081788, g_loss: 3.01410770, rnn_loss: 0.00000000\n",
      " ** Epoch 379 took 81.766298s\n",
      "Epoch: [380/2000] time: 0.5912s, d_loss: 0.64931399, g_loss: 3.97730494, rnn_loss: 0.00000000\n",
      " ** Epoch 380 took 82.146396s\n",
      "Epoch: [381/2000] time: 0.5913s, d_loss: 0.44610438, g_loss: 0.08829652, rnn_loss: 0.00000000\n",
      " ** Epoch 381 took 81.951928s\n",
      "Epoch: [382/2000] time: 0.5936s, d_loss: 0.12721774, g_loss: 3.53917360, rnn_loss: 0.00000000\n",
      " ** Epoch 382 took 81.936069s\n",
      "Epoch: [383/2000] time: 0.5982s, d_loss: 0.27995417, g_loss: 4.10195780, rnn_loss: 0.00000000\n",
      " ** Epoch 383 took 81.820444s\n",
      "Epoch: [384/2000] time: 0.5941s, d_loss: 0.30761230, g_loss: 2.02911210, rnn_loss: 0.00000000\n",
      " ** Epoch 384 took 82.144572s\n",
      "Epoch: [385/2000] time: 0.5938s, d_loss: 0.30080530, g_loss: 3.11702061, rnn_loss: 0.00000000\n",
      " ** Epoch 385 took 81.775643s\n",
      "Epoch: [386/2000] time: 0.5992s, d_loss: 0.40734527, g_loss: 3.87607574, rnn_loss: 0.00000000\n",
      " ** Epoch 386 took 81.907024s\n",
      "Epoch: [387/2000] time: 0.5898s, d_loss: 0.28857327, g_loss: 3.96280742, rnn_loss: 0.00000000\n",
      " ** Epoch 387 took 81.829150s\n",
      "Epoch: [388/2000] time: 0.5934s, d_loss: 0.64493185, g_loss: 4.58644390, rnn_loss: 0.00000000\n",
      " ** Epoch 388 took 82.077259s\n",
      "Epoch: [389/2000] time: 0.5890s, d_loss: 1.47075641, g_loss: 0.39265960, rnn_loss: 0.00000000\n",
      " ** Epoch 389 took 82.085798s\n",
      "Epoch: [390/2000] time: 0.5926s, d_loss: 0.03245663, g_loss: 2.82970190, rnn_loss: 0.00000000\n",
      " ** Epoch 390 took 81.856656s\n",
      "Epoch: [391/2000] time: 0.5916s, d_loss: 0.23990214, g_loss: 2.24176502, rnn_loss: 0.00000000\n",
      " ** Epoch 391 took 81.848615s\n",
      "Epoch: [392/2000] time: 0.5925s, d_loss: 1.22896767, g_loss: 3.87445569, rnn_loss: 0.00000000\n",
      " ** Epoch 392 took 82.082896s\n",
      "Epoch: [393/2000] time: 0.5958s, d_loss: 1.03844047, g_loss: 4.07687187, rnn_loss: 0.00000000\n",
      " ** Epoch 393 took 81.975866s\n",
      "Epoch: [394/2000] time: 0.5956s, d_loss: 0.36368001, g_loss: 1.50592995, rnn_loss: 0.00000000\n",
      " ** Epoch 394 took 81.788181s\n",
      "Epoch: [395/2000] time: 0.5953s, d_loss: 0.78204507, g_loss: 4.21817207, rnn_loss: 0.00000000\n",
      " ** Epoch 395 took 82.267642s\n",
      "Epoch: [396/2000] time: 0.5933s, d_loss: 0.35969225, g_loss: 1.93624079, rnn_loss: 0.00000000\n",
      " ** Epoch 396 took 81.917818s\n",
      "Epoch: [397/2000] time: 0.5937s, d_loss: 0.80676013, g_loss: 3.77045727, rnn_loss: 0.00000000\n",
      " ** Epoch 397 took 82.049634s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [398/2000] time: 0.5915s, d_loss: 0.20178831, g_loss: 0.42894322, rnn_loss: 0.00000000\n",
      " ** Epoch 398 took 81.832989s\n",
      "Epoch: [399/2000] time: 0.6052s, d_loss: 0.57507801, g_loss: 4.08513308, rnn_loss: 0.00000000\n",
      " ** Epoch 399 took 82.202709s\n",
      "Epoch: [400/2000] time: 0.6074s, d_loss: 0.59442824, g_loss: 4.08554173, rnn_loss: 0.00000000\n",
      " ** Epoch 400 took 82.092720s\n",
      "The checkpoint has been created.\n",
      "[*] Save checkpoints SUCCESS!\n",
      "Epoch: [401/2000] time: 0.5956s, d_loss: 0.58105898, g_loss: -0.27271831, rnn_loss: 0.00000000\n",
      " ** Epoch 401 took 81.984270s\n",
      "Epoch: [402/2000] time: 0.5950s, d_loss: 0.36699152, g_loss: 4.09683132, rnn_loss: 0.00000000\n",
      " ** Epoch 402 took 81.948226s\n",
      "Epoch: [403/2000] time: 0.6141s, d_loss: 0.03241655, g_loss: 1.96162391, rnn_loss: 0.00000000\n",
      " ** Epoch 403 took 82.220506s\n",
      "Epoch: [404/2000] time: 0.5936s, d_loss: 0.40260017, g_loss: 4.47894573, rnn_loss: 0.00000000\n",
      " ** Epoch 404 took 82.135648s\n",
      "Epoch: [405/2000] time: 0.5941s, d_loss: 0.84495294, g_loss: 3.82357597, rnn_loss: 0.00000000\n",
      " ** Epoch 405 took 81.961012s\n",
      "Epoch: [406/2000] time: 0.5927s, d_loss: 0.00571912, g_loss: 2.21611786, rnn_loss: 0.00000000\n",
      " ** Epoch 406 took 81.861101s\n",
      "Epoch: [407/2000] time: 0.5973s, d_loss: 0.51075077, g_loss: 0.36370742, rnn_loss: 0.00000000\n",
      " ** Epoch 407 took 82.247596s\n",
      "Epoch: [408/2000] time: 0.5974s, d_loss: 0.92128801, g_loss: 0.13598627, rnn_loss: 0.00000000\n",
      " ** Epoch 408 took 82.576595s\n",
      "Epoch: [409/2000] time: 0.5914s, d_loss: 0.56555873, g_loss: 0.67393219, rnn_loss: 0.00000000\n",
      " ** Epoch 409 took 82.082647s\n",
      "Epoch: [410/2000] time: 0.6081s, d_loss: 0.43859786, g_loss: 1.80806363, rnn_loss: 0.00000000\n",
      " ** Epoch 410 took 82.225090s\n",
      "Epoch: [411/2000] time: 0.5897s, d_loss: 0.25378001, g_loss: 0.54695791, rnn_loss: 0.00000000\n",
      " ** Epoch 411 took 82.009003s\n",
      "Epoch: [412/2000] time: 0.5921s, d_loss: 1.46224356, g_loss: 3.50035930, rnn_loss: 0.00000000\n",
      " ** Epoch 412 took 82.055604s\n",
      "Epoch: [413/2000] time: 0.5955s, d_loss: 0.42139217, g_loss: 4.44738483, rnn_loss: 0.00000000\n",
      " ** Epoch 413 took 82.006679s\n",
      "Epoch: [414/2000] time: 0.5975s, d_loss: 0.81564385, g_loss: 4.37777328, rnn_loss: 0.00000000\n",
      " ** Epoch 414 took 82.228876s\n",
      "Epoch: [415/2000] time: 0.5980s, d_loss: 0.42942283, g_loss: 1.39593577, rnn_loss: 0.00000000\n",
      " ** Epoch 415 took 82.072005s\n",
      "Epoch: [416/2000] time: 0.5995s, d_loss: 0.18701264, g_loss: 2.83623791, rnn_loss: 0.00000000\n",
      " ** Epoch 416 took 82.211276s\n",
      "Epoch: [417/2000] time: 0.5923s, d_loss: 0.24319699, g_loss: 0.62921166, rnn_loss: 0.00000000\n",
      " ** Epoch 417 took 81.984350s\n",
      "Epoch: [418/2000] time: 0.5906s, d_loss: 0.37287992, g_loss: 2.59101677, rnn_loss: 0.00000000\n",
      " ** Epoch 418 took 82.093969s\n",
      "Epoch: [419/2000] time: 0.5897s, d_loss: 0.73220158, g_loss: 0.44077200, rnn_loss: 0.00000000\n",
      " ** Epoch 419 took 81.984665s\n",
      "Epoch: [420/2000] time: 0.5935s, d_loss: 0.16160035, g_loss: 3.80066013, rnn_loss: 0.00000000\n",
      " ** Epoch 420 took 81.897521s\n",
      "Epoch: [421/2000] time: 0.5884s, d_loss: 0.89037120, g_loss: 0.26007378, rnn_loss: 0.00000000\n",
      " ** Epoch 421 took 81.826423s\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "model = Text2Img()\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement = True\n",
    "sess = tf.Session(config=config)\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "saver = tf.train.Saver(var_list=tf.global_variables(), max_to_keep=10)\n",
    "ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "    loader = tf.train.Saver(var_list=tf.global_variables())\n",
    "    load_step = int(os.path.basename(ckpt.model_checkpoint_path).split('-')[1])\n",
    "    load(loader, sess, ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    print('no checkpoints find.')\n",
    "\n",
    "n_epoch = 2000\n",
    "n_batch_epoch = int(n_images_train / batch_size)\n",
    "for epoch in range(n_epoch + 1):\n",
    "    start_time = time.time()\n",
    "    for step in range(n_batch_epoch):\n",
    "        step_time = time.time()\n",
    "        idexs = get_random_int(min=0, max=n_captions_train-1, number=batch_size)\n",
    "        b_real_caption = train_captions[idexs]\n",
    "        b_real_images = train_images[np.floor(np.asarray(idexs).astype('float') / n_captions_per_image).astype('int')]\n",
    "        idexs = get_random_int(min=0, max=n_captions_train-1, number=batch_size)\n",
    "        b_wrong_caption = train_captions[idexs]\n",
    "        idexs2 = get_random_int(min=0, max=n_images_train-1, number=batch_size)\n",
    "        b_wrong_images = train_images[idexs2]\n",
    "        b_z = np.random.normal(loc=0.0, scale=1.0, size=(batch_size, z_dim)).astype(np.float32)\n",
    "        b_real_images = threading_data(b_real_images, prepro_img, mode='train')\n",
    "        b_wrong_images = threading_data(b_wrong_images, prepro_img, mode='train')        \n",
    "        if epoch < 100:\n",
    "            errRNN, _ = sess.run([model.rnn_loss, model.rnn_optim], feed_dict={\n",
    "                                            model.t_real_image : b_real_images,\n",
    "                                            model.t_wrong_image : b_wrong_images,\n",
    "                                            model.t_real_caption : b_real_caption,\n",
    "                                            model.t_wrong_caption : b_wrong_caption})\n",
    "        else:\n",
    "            errRNN = 0\n",
    "        errD, _ = sess.run([model.d_loss, model.d_optim], feed_dict={\n",
    "                            model.t_real_image : b_real_images,\n",
    "                            model.t_wrong_caption : b_wrong_caption,\n",
    "                            model.t_real_caption : b_real_caption,\n",
    "                            model.t_z : b_z})\n",
    "        errG, _ = sess.run([model.g_loss, model.g_optim], feed_dict={\n",
    "                            model.t_real_caption : b_real_caption,\n",
    "                            model.t_z : b_z})\n",
    "    print(\"Epoch: [%d/%d] time: %4.4fs, d_loss: %.8f, g_loss: %.8f, rnn_loss: %.8f\" \\\n",
    "                        % (epoch, n_epoch, time.time() - step_time, errD, errG, errRNN))\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(\" ** Epoch %d took %fs\" % (epoch, time.time()-start_time))\n",
    "        img_gen, rnn_out = sess.run([model.net_g.outputs, model.net_rnn.outputs], feed_dict={\n",
    "                                        model.t_real_caption : sample_sentence,\n",
    "                                        model.t_z : sample_seed})\n",
    "        save_images(img_gen, [ni, ni], 'train_samples_last_gan/train_{:02d}.png'.format(epoch))\n",
    "    if (epoch != 0) and (epoch % 50) == 0:\n",
    "        save(saver, sess, checkpoint_dir, epoch)\n",
    "        print(\"[*] Save checkpoints SUCCESS!\")\n",
    "checkpoint_path = os.path.join(cfg.CHECKPOINT_DIR, cfg.CHECKPOINT_NAME)\n",
    "saver.save(sess, checkpoint_path, global_step=epoch)\n",
    "print('The checkpoint has been created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_r_precision_data():\n",
    "    caption_ids = np.reshape(np.asarray(test_dataset.captions_ids), (-1, cfg.TEXT.WORDS_NUM))\n",
    "    captions_ids_wrong = np.reshape(test_dataset.random_wrong_captions(), (-1, cfg.WRONG_CAPTION, cfg.TEXT.WORDS_NUM))\n",
    "\n",
    "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # load the trained checkpoint\n",
    "    checkpoint_dir = cfg.CHECKPOINT_DIR\n",
    "    if checkpoint_dir is not None:\n",
    "        loader = tf.train.Saver(var_list=tf.global_variables())\n",
    "        ckpt_path = os.path.join(cfg.CHECKPOINT_DIR, CHECKPOINT_NAME)\n",
    "        loader.restore(sess, ckpt_path)\n",
    "        print(\"Restored model parameters from {}\".format(ckpt_path))\n",
    "    else:\n",
    "        print('no checkpoints find.')\n",
    "\n",
    "    n_caption_test = len(caption_ids)\n",
    "    num_batches = n_caption_test // cfg.BATCH_SIZE\n",
    "\n",
    "    true_cnn_features = np.zeros((num_batches, cfg.BATCH_SIZE, cfg.TEXT.EMBEDDING_DIM), dtype=float)\n",
    "    true_rnn_features = np.zeros((num_batches, cfg.BATCH_SIZE, cfg.TEXT.EMBEDDING_DIM), dtype=float)\n",
    "    wrong_rnn_features = np.zeros((num_batches, cfg.WRONG_CAPTION, cfg.BATCH_SIZE, cfg.TEXT.EMBEDDING_DIM), dtype=float)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        test_cap = caption_ids[i * cfg.BATCH_SIZE: (i + 1) * cfg.BATCH_SIZE]\n",
    "\n",
    "        z = np.random.normal(loc=0.0, scale=1.0, size=(cfg.BATCH_SIZE, cfg.GAN.Z_DIM)).astype(np.float32)\n",
    "        \n",
    "        rnn_features = sess.run(rnn_encoder.outputs, feed_dict={t_real_caption: test_cap})\n",
    "        gen = sess.run(generator.outputs, feed_dict={t_real_caption: test_cap, t_z: z})\n",
    "        cnn_features = sess.run(cnn_encoder.outputs, feed_dict={t_real_image: gen})\n",
    "\n",
    "        true_cnn_features[i] = cnn_features\n",
    "        true_rnn_features[i] = rnn_features\n",
    "\n",
    "        for per_wrong_caption in range(cfg.WRONG_CAPTION):\n",
    "            test_cap = captions_ids_wrong[i * cfg.BATCH_SIZE: (i + 1) * cfg.BATCH_SIZE]\n",
    "            rnn_features = sess.run(rnn_encoder.outputs, feed_dict={t_real_caption: test_cap[:, per_wrong_caption]})\n",
    "            wrong_rnn_features[i, per_wrong_caption] = rnn_features\n",
    "    \n",
    "    # if exists, remove the existing file first\n",
    "    try:\n",
    "        os.remove(os.path.join(cfg.R_PRECISION_DIR, cfg.R_PRECISION_FILE))\n",
    "    except OSError:\n",
    "        pass\n",
    "    np.savez(os.path.join(cfg.R_PRECISION_DIR, cfg.R_PRECISION_FILE), true_cnn=true_cnn_features, true_rnn=true_rnn_features,\n",
    "             wrong_rnn=wrong_rnn_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inception_score_data():\n",
    "    caption_ids = np.reshape(np.asarray(test_dataset.captions_ids),\n",
    "                             (-1, cfg.TEXT.CAPTIONS_PER_IMAGE, cfg.TEXT.WORDS_NUM))\n",
    "    \n",
    "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    checkpoint_dir = cfg.CHECKPOINT_DIR\n",
    "    if checkpoint_dir is not None:\n",
    "        loader = tf.train.Saver(var_list=tf.global_variables())\n",
    "        ckpt_path = os.path.join(cfg.CHECKPOINT_DIR, cfg.CHECKPOINT_NAME)\n",
    "        loader.restore(sess, ckpt_path)\n",
    "        print(\"Restored model parameters from {}\".format(ckpt_path))\n",
    "    else:\n",
    "        print('no checkpoints find.')\n",
    "\n",
    "    n_caption_test = len(caption_ids)\n",
    "    num_batches = n_caption_test // cfg.BATCH_SIZE\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        for per_caption in range(cfg.TEXT.CAPTIONS_PER_IMAGE):\n",
    "            test_cap = caption_ids[i * cfg.BATCH_SIZE: (i + 1) * cfg.BATCH_SIZE, per_caption]\n",
    "            test_directory = test_dataset.filenames[i * cfg.BATCH_SIZE: (i + 1) * cfg.BATCH_SIZE]\n",
    "\n",
    "            z = np.random.normal(loc=0.0, scale=1.0, size=(cfg.BATCH_SIZE, cfg.GAN.Z_DIM)).astype(np.float32)\n",
    "            gen = sess.run(generator.outputs, feed_dict={t_real_caption: test_cap, t_z: z})\n",
    "            \n",
    "            for j in range(cfg.BATCH_SIZE):\n",
    "                if not os.path.exists(os.path.join(cfg.TEST.GENERATED_TEST_IMAGES, test_directory[j].split('/')[0])):\n",
    "                    os.mkdir(os.path.join(cfg.TEST.GENERATED_TEST_IMAGES, test_directory[j].split('/')[0]))\n",
    "\n",
    "                scipy.misc.imsave(os.path.join(cfg.TEST.GENERATED_TEST_IMAGES, test_directory[j] + '_{}.png'.format(per_caption)), gen[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_r_precision_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_inception_score_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Measure Inception score and R-precision of given test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After set the config file as 'eval_birds.yml' and run the 'generate_inception_score_data()' and 'generate_r_precision_data()', the synthesized images based on given captions and set of image and caption features should be saved inside a 'evaluation' folder, specifically in 'evaluation/generated_images/..' and as 'evaluation/r_precision.npz' respectively.\n",
    "\n",
    "**Then, go to the 'evaluation' folder and run each 'inception_score.ipynb' and 'r_precision.ipynb' file in order to measure inception score and r-precision score.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
